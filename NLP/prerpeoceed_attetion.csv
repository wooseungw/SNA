"Title","Journal","Year"
"ChatGPT for good? On opportunities and challenges of large language models for education","Learning and individual ","2023"
"[HTML][HTML] Attention mechanisms in computer vision: A survey","Computational visual ","2022"
"High-resolution image synthesis with latent diffusion models","Proceedings of the ","2022"
"Robust speech recognition via large-scale weak supervision","International ","2023"
"A convnet for the 2020s","Proceedings of the ","2022"
"Palm: Scaling language modeling with pathways","arXiv preprint arXiv ","2022"
"Flamingo: a visual language model for few-shot learning","Advances in ","2022"
" Instant neural graphics primitives with a multiresolution hash encoding","ACM Transactions on Graphics ","2022"
"Glide: Towards photorealistic image generation and editing with text-guided diffusion models","arXiv preprint arXiv ","2021"
" Scaling autoregressive models for content-rich text-to-image generation","arXiv preprint arXiv ","2022"
"Large language models are zero-shot reasoners","Advances in neural ","2022"
"Masked-attention mask transformer for universal image segmentation","Proceedings of the ","2022"
"Data2vec: A general framework for self-supervised learning in speech, vision and language"," on Machine Learning","2022"
"Bloom: A 176b-parameter open-access multilingual language model","arXiv preprint arXiv ","2022"
"Laion-5b: An open large-scale dataset for training next generation image-text models","Advances in ","2022"
"Exploring plain vision transformer backbones for object detection","European Conference on Computer Vision","2022"
"Visual prompt tuning"," on Computer Vision","2022"
"Card: Classification and regression diffusion models","Advances in Neural Information ","2022"
"Swin transformer v2: Scaling up capacity and resolution","Proceedings of the ","2022"
"Coca: Contrastive captioners are image-text foundation models","arXiv preprint arXiv ","2022"
"Restormer: Efficient transformer for high-resolution image restoration","Proceedings of the ","2022"
"Lamda: Language models for dialog applications","arXiv preprint arXiv ","2022"
"Scaling up your kernels to 31x31: Revisiting large kernel design in cnns","Proceedings of the IEEE ","2022"
"Conditional prompt learning for vision-language models","Proceedings of the IEEE ","2022"
"Masked autoencoders are scalable vision learners","Proceedings of the ","2022"
"Improving language models by retrieving from trillions of tokens","International ","2022"
"Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training","Advances in neural ","2022"
"[HTML][HTML] ¡°So what if ChatGPT wrote it?¡± Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research ¡¦","International Journal of ","2023"
"Bevformer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers","European conference on ","2022"
"Scalable diffusion models with transformers","Proceedings of the IEEE/CVF ","2023"
"Masked feature prediction for self-supervised visual pre-training","Proceedings of the ","2022"
"Diffusiondet: Diffusion model for object detection","Proceedings of the IEEE ","2023"
"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models","arXiv preprint arXiv ","2022"
"Competition-level code generation with alphacode","Science","2022"
"[HTML][HTML] Discovering faster matrix multiplication algorithms with reinforcement learning","Nature","2022"
"A generalist agent","arXiv preprint arXiv ","2022"
"Emergent abilities of large language models","arXiv preprint arXiv ","2022"
"Evolutionary-scale prediction of atomic-level protein structure with a language model","Science","2023"
"Masked autoencoders as spatiotemporal learners","Advances in neural ","2022"
"Segment anything","arXiv preprint arXiv ","2023"
"Survey of hallucination in natural language generation","ACM Computing ","2023"
"Bytetrack: Multi-object tracking by associating every detection box"," on Computer Vision","2022"
"Grounded language-image pre-training","Proceedings of the ","2022"
"Sparks of artificial general intelligence: Early experiments with gpt-4","arXiv preprint arXiv ","2023"
"Mvitv2: Improved multiscale vision transformers for classification and detection","Proceedings of the ","2022"
"Simple baselines for image restoration","European Conference on Computer ","2022"
"Diffusion-lm improves controllable text generation","Advances in Neural ","2022"
"Do as i can, not as i say: Grounding language in robotic affordances","arXiv preprint arXiv ","2022"
"Wavlm: Large-scale self-supervised pre-training for full stack speech processing","IEEE Journal of ","2022"
"Metaformer is actually what you need for vision","Proceedings of the ","2022"
"Smoothquant: Accurate and efficient post-training quantization for large language models","International ","2023"
"Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation","Proceedings of the ","2023"
"Flava: A foundational language and vision alignment model","Proceedings of the ","2022"
"Are transformers effective for time series forecasting?"," of the AAAI conference on artificial ","2023"
"Learning to prompt for vision-language models","International Journal of Computer Vision","2022"
"Swinir: Image restoration using swin transformer","Proceedings of the ","2021"
"Vector quantized diffusion model for text-to-image synthesis","Proceedings of the ","2022"
"TPH-YOLOv5: Improved YOLOv5 based on transformer prediction head for object detection on drone-captured scenarios","Proceedings of the IEEE ","2021"
"Clip-adapter: Better vision-language models with feature adapters","International Journal of ","2023"
"Inception transformer","Advances in Neural ","2022"
"Do vision transformers see like convolutional neural networks?","Advances in ","2021"
"On the opportunities and risks of foundation models","arXiv preprint arXiv ","2021"
"Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting"," on Machine Learning","2022"
"Petr: Position embedding transformation for multi-view 3d object detection","European Conference on Computer ","2022"
"[HTML][HTML] Visual attention network","Computational Visual Media","2023"
"Make-a-scene: Scene-based text-to-image generation with human priors"," on Computer Vision","2022"
"Transfusion: Robust lidar-camera fusion for 3d object detection with transformers","Proceedings of the ","2022"
"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing","ACM Computing ","2023"
"Florence: A new foundation model for computer vision","arXiv preprint arXiv ","2021"
"Palette: Image-to-image diffusion models","ACM SIGGRAPH 2022 ","2022"
"Lit: Zero-shot transfer with locked-image text tuning","Proceedings of the ","2022"
"Robust deep learning–based protein sequence design using ProteinMPNN","Science","2022"
"Dino: Detr with improved denoising anchor boxes for end-to-end object detection","arXiv preprint arXiv ","2022"
"[HTML][HTML] Highly accurate protein structure prediction with AlphaFold","Nature","2021"
"Point-bert: Pre-training 3d point cloud transformers with masked point modeling","Proceedings of the ","2022"
"Eva: Exploring the limits of masked visual representation learning at scale","Proceedings of the ","2023"
"Per-pixel classification is not all you need for semantic segmentation","Advances in Neural ","2021"
"Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model","arXiv preprint arXiv ","2022"
"Dn-detr: Accelerate detr training by introducing query denoising","Proceedings of the ","2022"
"Blended diffusion for text-driven editing of natural images","Proceedings of the IEEE ","2022"
"An end-to-end transformer model for 3d object detection","Proceedings of the IEEE/CVF ","2021"
"Ego4d: Around the world in 3,000 hours of egocentric video","Proceedings of the ","2022"
"Early convolutions help transformers see better","Advances in neural ","2021"
"Denseclip: Language-guided dense prediction with context-aware prompting","Proceedings of the ","2022"
"Artificial intelligence in histopathology: enhancing cancer research and clinical oncology","Nature cancer","2022"
"Beit: Bert pre-training of image transformers","arXiv preprint arXiv:2106.08254","2021"
"Accurate prediction of protein structures and interactions using a three-track neural network","Science","2021"
"[HTML][HTML] Pvt v2: Improved baselines with pyramid vision transformer","Computational Visual ","2022"
"Video swin transformer","Proceedings of the ","2022"
"Internimage: Exploring large-scale vision foundation models with deformable convolutions","Proceedings of the ","2023"
"Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting","Advances in Neural ","2021"
"Coatnet: Marrying convolution and attention for all data sizes","Advances in neural information ","2021"
"[HTML][HTML] Large language models encode clinical knowledge","Nature","2023"
"Groupvit: Semantic segmentation emerges from text supervision","Proceedings of the ","2022"
"Lora: Low-rank adaptation of large language models","arXiv preprint arXiv ","2021"
"Cswin transformer: A general vision transformer backbone with cross-shaped windows","Proceedings of the ","2022"
"Maxvit: Multi-axis vision transformer","European conference on ","2022"
"Do transformers really perform badly for graph representation?","Advances in ","2021"
"Uformer: A general u-shaped transformer for image restoration","Proceedings of the ","2022"
"Scaling vision transformers","Proceedings of the ","2022"
"Zero-shot text-guided object generation with dream fields","Proceedings of the ","2022"
"SegFormer: Simple and efficient design for semantic segmentation with transformers","Advances in ","2021"
"Decision transformer: Reinforcement learning via sequence modeling","Advances in neural ","2021"
"[HTML][HTML] A survey of transformers","AI Open","2022"
"Scientific discovery in the age of artificial intelligence","Nature","2023"
"An empirical study of training end-to-end vision-and-language transformers","Proceedings of the ","2022"
"Obtaining genetics insights from deep learning via explainable artificial intelligence","Nature Reviews ","2023"
"Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning","Advances in ","2022"
"Semi-supervised semantic segmentation with cross pseudo supervision","Proceedings of the IEEE ","2021"
"Quantum advantage in learning from experiments","Science","2022"
"StyleGAN-NADA: CLIP-guided domain adaptation of image generators","ACM Transactions on ","2022"
"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents"," on Machine Learning","2022"
"Maxim: Multi-axis mlp for image processing","Proceedings of the ","2022"
"Masked siamese networks for label-efficient learning"," on Computer Vision","2022"
"Swin-unet: Unet-like pure transformer for medical image segmentation","European conference on ","2022"
"Prompt-to-prompt image editing with cross attention control","arXiv preprint arXiv ","2022"
"Diffusion models beat gans on image synthesis","Advances in neural information ","2021"
"Segmenter: Transformer for semantic segmentation","Proceedings of the ","2021"
"Masked autoencoders for point cloud self-supervised learning","European conference on ","2022"
"Multi-concept customization of text-to-image diffusion","Proceedings of the ","2023"
"A comprehensive survey on graph anomaly detection with deep learning"," on Knowledge and ","2021"
"Conditional detr for fast training convergence","Proceedings of the ","2021"
"Offline reinforcement learning as one big sequence modeling problem","Advances in neural information ","2021"
"Mlp-mixer: An all-mlp architecture for vision","Advances in neural ","2021"
"Vlmo: Unified vision-language pre-training with mixture-of-modality-experts","Advances in ","2022"
"Self-supervised learning in medicine and healthcare","Nature Biomedical Engineering","2022"
"Video pretraining (vpt): Learning to act by watching unlabeled online videos","Advances in ","2022"
"Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech","International Conference on Machine ","2021"
"Emerging properties in self-supervised vision transformers","Proceedings of the ","2021"
"Recent advances in natural language processing via large pre-trained language models: A survey","ACM Computing ","2023"
"SwinFusion: Cross-domain long-range learning for general image fusion via swin transformer","IEEE/CAA Journal of ","2022"
"Intriguing properties of vision transformers","Advances in ","2021"
"Flashattention: Fast and memory-efficient exact attention with io-awareness","Advances in Neural ","2022"
"Simvlm: Simple visual language model pretraining with weak supervision","arXiv preprint arXiv ","2021"
"Vision transformer with deformable attention","Proceedings of the IEEE ","2022"
"Multimodal few-shot learning with frozen language models","Advances in ","2021"
"How attentive are graph attention networks?","arXiv preprint arXiv:2105.14491","2021"
"Twins: Revisiting the design of spatial attention in vision transformers","Advances in ","2021"
"Simcse: Simple contrastive learning of sentence embeddings","arXiv preprint arXiv:2104.08821","2021"
"Diffusion models in vision: A survey","IEEE Transactions on ","2023"
"The power of scale for parameter-efficient prompt tuning","arXiv preprint arXiv:2104.08691","2021"
"Pay attention to mlps","Advances in Neural Information ","2021"
"Voxel transformer for 3d object detection","Proceedings of the ","2021"
"Cogview: Mastering text-to-image generation via transformers","Advances in ","2021"
"Transformers in medical imaging: A survey","Medical Image ","2023"
"Cmt: Convolutional neural networks meet vision transformers","Proceedings of the ","2022"
"Scaling vision transformers to gigapixel images via hierarchical self-supervised learning","Proceedings of the ","2022"
"Mdetr-modulated detection for end-to-end multi-modal understanding","Proceedings of the ","2021"
"Winoground: Probing vision and language models for visio-linguistic compositionality","Proceedings of the ","2022"
"Multiscale vision transformers","Proceedings of the ","2021"
"Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation","arXiv preprint arXiv:2109.00859","2021"
"Perceiver-actor: A multi-task transformer for robotic manipulation","Conference on Robot ","2023"
"Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text","Advances in ","2021"
"Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation","Proceedings of the IEEE/CVF ","2022"
"XLS-R: Self-supervised cross-lingual speech representation learning at scale","arXiv preprint arXiv ","2021"
"Image super-resolution via iterative refinement"," on Pattern Analysis ","2022"
"Cvt: Introducing convolutions to vision transformers","Proceedings of the ","2021"
"Vivit: A video vision transformer","Proceedings of the ","2021"
"Scaling language-image pre-training via masking","Proceedings of the ","2023"
"A review on the attention mechanism of deep learning","Neurocomputing","2021"
"Styleclip: Text-driven manipulation of stylegan imagery","Proceedings of the ","2021"
"Going deeper with image transformers","Proceedings of the ","2021"
"SpeechBrain: A general-purpose speech toolkit","arXiv preprint arXiv ","2021"
"Maskgit: Masked generative image transformer","Proceedings of the ","2022"
"Crossvit: Cross-attention multi-scale vision transformer for image classification","Proceedings of the IEEE/CVF ","2021"
"Swin transformer: Hierarchical vision transformer using shifted windows","Proceedings of the ","2021"
"ChatGPT and other large language models are double-edged swords","Radiology","2023"
"Clipcap: Clip prefix for image captioning","arXiv preprint arXiv:2111.09734","2021"
"Transformer tracking","Proceedings of the ","2021"
"Vision transformers for dense prediction","Proceedings of the IEEE ","2021"
"Learning inverse folding from millions of predicted structures","International ","2022"
"Dynamicvit: Efficient vision transformers with dynamic token sparsification","Advances in neural ","2021"
"[HTML][HTML] Skilful precipitation nowcasting using deep generative models of radar","Nature","2021"
"LoFTR: Detector-free local feature matching with transformers","Proceedings of the ","2021"
"Are transformers more robust than cnns?","Advances in neural ","2021"
"Mixformer: End-to-end tracking with iterative mixed attention","Proceedings of the IEEE ","2022"
"Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators","Nature machine ","2021"
"Unetr: Transformers for 3d medical image segmentation","Proceedings of the ","2022"
"A survey of quantization methods for efficient neural network inference","Low-Power Computer ","2022"
"Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing","arXiv preprint arXiv:2111.09543","2021"
"Convit: Improving vision transformers with soft convolutional inductive biases","International ","2021"
"Image super-resolution with non-local sparse attention"," of the IEEE/CVF Conference on ","2021"
"SpectralFormer: Rethinking hyperspectral image classification with transformers"," on Geoscience and ","2021"
"[HTML][HTML] ProtGPT2 is a deep unsupervised language model for protein design","Nature communications","2022"
"Conformer: Local features coupling global representations for visual recognition","Proceedings of the ","2021"
"Attention bottlenecks for multimodal fusion","Advances in ","2021"
"Stratified transformer for 3d point cloud segmentation","Proceedings of the ","2022"
"Hornet: Efficient high-order spatial interactions with recursive gated convolutions","Advances in Neural ","2022"
"A survey of modern deep learning based object detection models","Digital Signal ","2022"
"Detr3d: 3d object detection from multi-view images via 3d-to-2d queries"," on Robot Learning","2022"
"Mobile-former: Bridging mobilenet and transformer","Proceedings of the ","2022"
"Structured denoising diffusion models in discrete state-spaces","Advances in ","2021"
"Transformer in transformer","Advances in Neural ","2021"
"Learning spatio-temporal transformer for visual tracking","Proceedings of the IEEE ","2021"
"Frozen in time: A joint video and image encoder for end-to-end retrieval","Proceedings of the ","2021"
"Rethinking spatial dimensions of vision transformers","Proceedings of the ","2021"
"Zero-shot text-to-image generation","International ","2021"
"Pyramid vision transformer: A versatile backbone for dense prediction without convolutions","Proceedings of the ","2021"
"Social physics","Physics Reports","2022"
"Dynamic head: Unifying object detection heads with attentions","Proceedings of the ","2021"
"Pointr: Diverse point cloud completion with geometry-aware transformers","Proceedings of the ","2021"
"Perceiver: General perception with iterative attention","International ","2021"
"You only look one-level feature","Proceedings of the ","2021"
"Revisiting deep learning models for tabular data","Advances in Neural ","2021"
"Cliport: What and where pathways for robotic manipulation","Conference on Robot ","2022"
" Is space-time attention all you need for video understanding?","ICML","2021"
"Transunet: Transformers make strong encoders for medical image segmentation","arXiv preprint arXiv ","2021"
"Training spiking neural networks using lessons from deep learning","Proceedings of the ","2023"
"Deep neural networks and tabular data: A survey"," on Neural Networks ","2022"
"[HTML][HTML] How does ChatGPT perform on the United States medical licensing examination? The implications of large language models for medical education and ¡¦","JMIR Medical ","2023"
"Transformer meets tracker: Exploiting temporal context for robust visual tracking","Proceedings of the IEEE ","2021"
"[HTML][HTML] Effective gene expression prediction from sequence by integrating long-range interactions","Nature ","2021"
"Large language models generate functional protein sequences across diverse families","Nature ","2023"
"Adaptformer: Adapting vision transformers for scalable visual recognition","Advances in ","2022"
"Tokens-to-token vit: Training vision transformers from scratch on imagenet","Proceedings of the ","2021"
"Observation-centric sort: Rethinking sort for robust multi-object tracking","Proceedings of the ","2023"
"Bottleneck transformers for visual recognition","Proceedings of the ","2021"
"Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts","Proceedings of the ","2021"
"On the dangers of stochastic parrots: Can language models be too big?🦜","Proceedings of the 2021 ","2021"
"Transreid: Transformer-based object re-identification","Proceedings of the ","2021"
"Directed evolution: methodologies and applications","Chemical reviews","2021"
"Cogview2: Faster and better text-to-image generation via hierarchical transformers","Advances in Neural ","2022"
"Ibrnet: Learning multi-view image-based rendering","Proceedings of the ","2021"
"You only learn one representation: Unified network for multiple tasks","arXiv preprint arXiv:2105.04206","2021"
"Pointclip: Point cloud understanding by clip","Proceedings of the ","2022"
"Cross-view transformers for real-time map-view semantic segmentation"," of the IEEE/CVF conference on ","2022"
"Extract free dense labels from clip","European Conference on Computer Vision","2022"
"Multi-game decision transformers","Advances in ","2022"
"Vitae: Vision transformer advanced by exploring intrinsic inductive bias","Advances in neural ","2021"
"Recipe for a general, powerful, scalable graph transformer","Advances in ","2022"
"Ast: Audio spectrogram transformer","arXiv preprint arXiv:2104.01778","2021"
"Focal self-attention for local-global interactions in vision transformers","arXiv preprint arXiv ","2021"
"Incorporating convolution designs into visual transformers","Proceedings of the ","2021"
"Learning to prompt for continual learning","Proceedings of the ","2022"
"Diffusionclip: Text-guided diffusion models for robust image manipulation"," of the IEEE/CVF Conference on ","2022"
"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity","The Journal of Machine Learning ","2022"
"Barf: Bundle-adjusting neural radiance fields","Proceedings of the IEEE ","2021"
"Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning","Neurocomputing","2022"
"Learning transferable visual models from natural language supervision","International ","2021"
"Less is more: Clipbert for video-and-language learning via sparse sampling","Proceedings of the ","2021"
"Transformers in vision: A survey","ACM computing ","2022"
"Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers","Proceedings of the ","2021"
"Prefix-tuning: Optimizing continuous prompts for generation","arXiv preprint arXiv:2101.00190","2021"
"Levit: a vision transformer in convnet's clothing for faster inference","Proceedings of the ","2021"
"Perceiver io: A general architecture for structured inputs & outputs","arXiv preprint arXiv ","2021"
"Large language models struggle to learn long-tail knowledge","International ","2023"
"Localvit: Bringing locality to vision transformers","arXiv preprint arXiv ","2021"
"Transmil: Transformer based correlated multiple instance learning for whole slide image classification","Advances in neural ","2021"
"High-performance large-scale image recognition without normalization"," Conference on Machine ","2021"
"Training data-efficient image transformers & distillation through attention","International ","2021"
"Merlot: Multimodal neural script knowledge models","Advances in ","2021"
"Open-vocabulary object detection via vision and language knowledge distillation","arXiv preprint arXiv:2104.13921","2021"
"Transfuse: Fusing transformers and cnns for medical image segmentation","Medical Image Computing and Computer Assisted ","2021"
"Taming transformers for high-resolution image synthesis","Proceedings of the IEEE ","2021"
"Point transformer","Proceedings of the ","2021"
"Vision-language pre-training with triple contrastive learning","Proceedings of the ","2022"
"Informer: Beyond efficient transformer for long sequence time-series forecasting","Proceedings of the ","2021"
"A survey on vision transformer","IEEE transactions on ","2022"
"Convnext v2: Co-designing and scaling convnets with masked autoencoders","Proceedings of the ","2023"
"Pct: Point cloud transformer","Computational Visual ","2021"
"Trackformer: Multi-object tracking with transformers","Proceedings of the ","2022"
"Glipv2: Unifying localization and vision-language understanding","Advances in ","2022"
"Emergent analogical reasoning in large language models","Nature Human Behaviour","2023"
"Extracting training data from large language models","30th USENIX Security ","2021"
"Deep bidirectional language-knowledge graph pretraining","Advances in ","2022"
"Merlot reserve: Neural script knowledge through vision and language and sound","Proceedings of the ","2022"
"Deepvit: Towards deeper vision transformer","arXiv preprint arXiv ","2021"
"Vision transformer adapter for dense predictions","arXiv preprint arXiv ","2022"
"Understanding the robustness in vision transformers","International ","2022"
"Program synthesis with large language models","arXiv preprint arXiv ","2021"
"Pre-trained image processing transformer","Proceedings of the ","2021"
"Scaling local self-attention for parameter efficient visual backbones","Proceedings of the ","2021"
"Actionformer: Localizing moments of actions with transformers","European Conference on Computer Vision","2022"
"BioGPT: generative pre-trained transformer for biomedical text generation and mining","Briefings in ","2022"
"Multiview transformers for video recognition","Proceedings of the ","2022"
"Multi-modal fusion transformer for end-to-end autonomous driving","Proceedings of the IEEE ","2021"
"Sparse r-cnn: End-to-end object detection with learnable proposals","Proceedings of the ","2021"
"Scaling vision with sparse mixture of experts","Advances in ","2021"
"Rethinking semantic segmentation: A prototype view","Proceedings of the ","2022"
"Unifying vision-and-language tasks via text generation","International Conference on ","2021"
"Deit iii: Revenge of the vit","European Conference on Computer Vision","2022"
"[HTML][HTML] Multimodal biomedical AI","Nature Medicine","2022"
"Real-world robot learning with masked visual pre-training"," on Robot Learning","2023"
"Videoclip: Contrastive pre-training for zero-shot video-text understanding","arXiv preprint arXiv ","2021"
"Graph neural network for traffic forecasting: A survey","Expert Systems with Applications","2022"
"Unsupervised speech recognition","Advances in Neural ","2021"
"Fastnerf: High-fidelity neural rendering at 200fps","Proceedings of the ","2021"
"End-to-end video instance segmentation with transformers","Proceedings of the ","2021"
"Vision gnn: An image is worth graph of nodes","Advances in Neural ","2022"
"D-nerf: Neural radiance fields for dynamic scenes","Proceedings of the ","2021"
"Resmlp: Feedforward networks for image classification with data-efficient training"," on Pattern Analysis ","2022"
"[HTML][HTML] A review of uncertainty quantification in deep learning: Techniques, applications and challenges","Information fusion","2021"
"Davit: Dual attention vision transformers","European Conference on ","2022"
"[HTML][HTML] Geometry-enhanced molecular representation learning for property prediction","Nature Machine ","2022"
"Fcanet: Frequency channel attention networks","Proceedings of the IEEE/CVF ","2021"
"Conditional positional encodings for vision transformers","arXiv preprint arXiv ","2021"
"Unified pre-training for program understanding and generation","arXiv preprint arXiv ","2021"
"Ota: Optimal transport assignment for object detection","Proceedings of the IEEE ","2021"
"Transformers in time series: A survey","arXiv preprint arXiv ","2022"
"Remote sensing image change detection with transformers","IEEE Transactions on Geoscience and ","2021"
"Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks","The Journal of Machine ","2021"
"Dynamic detr: End-to-end object detection with dynamic attention","Proceedings of the ","2021"
"Global filter networks for image classification","Advances in neural ","2021"
"Unified contrastive learning in image-text-label space","Proceedings of the ","2022"
"Underspecification presents challenges for credibility in modern machine learning","The Journal of Machine ","2022"
"mT5: A massively multilingual pre-trained text-to-text transformer","arXiv preprint arXiv ","2020"
"Neural operator: Learning maps between function spaces","arXiv preprint arXiv ","2021"
"End-to-end human pose and mesh reconstruction with transformers"," of the IEEE/CVF conference on ","2021"
"Codexglue: A machine learning benchmark dataset for code understanding and generation","arXiv preprint arXiv ","2021"
"Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction","Proceedings of the ","2021"
"Dynamic neural networks: A survey","IEEE Transactions on ","2021"
"CryoDRGN: reconstruction of heterogeneous cryo-EM structures using neural networks","Nature methods","2021"
"Video transformer network","Proceedings of the ","2021"
"Locating and editing factual associations in GPT","Advances in Neural ","2022"
"Spectral–spatial feature tokenization transformer for hyperspectral image classification","IEEE Transactions on ","2022"
"Torsional diffusion for molecular conformer generation","Advances in Neural ","2022"
"Diverse part discovery: Occluded person re-identification with part-aware transformer","Proceedings of the ","2021"
"Understanding robustness of transformers for image classification","Proceedings of the ","2021"
"Rethinking graph transformers with spectral attention","Advances in ","2021"
"Deformable detr: Deformable transformers for end-to-end object detection","arXiv preprint arXiv:2010.04159","2020"
"Harnessing multimodal data integration to advance precision oncology","Nature Reviews ","2022"
"V2x-vit: Vehicle-to-everything cooperative perception with vision transformer","European conference on ","2022"
"Revisiting resnets: Improved training and scaling strategies","Advances in ","2021"
"Human-level play in the game of Diplomacy by combining language models with strategic reasoning","Science","2022"
"Using cognitive psychology to understand GPT-3","Proceedings of the National Academy of ","2023"
"Transbts: Multimodal brain tumor segmentation using transformer","Medical Image Computing ","2021"
"Embracing single stride 3d object detector with sparse transformer","Proceedings of the ","2022"
"DeepLoc 2.0: multi-label subcellular localization prediction using protein language models","Nucleic Acids ","2022"
"Adbench: Anomaly detection benchmark","Advances in Neural ","2022"
"Transformer interpretability beyond attention visualization"," of the IEEE/CVF conference on ","2021"
"Rethinking attention with performers","arXiv preprint arXiv ","2020"
"Combining machine learning and computational chemistry for predictive insights into chemical systems","Chemical ","2021"
"Mat: Mask-aware transformer for large hole image inpainting","Proceedings of the ","2022"
"Nüwa: Visual synthesis pre-training for neural visual world creation","European conference on ","2022"
"Co-scale conv-attentional image transformers","Proceedings of the IEEE/CVF ","2021"
"[HTML][HTML] Pre-trained models: Past, present and future","AI Open","2021"
"[HTML][HTML] Intelligent metasurfaces: control, communication and computing","Elight","2022"
"U-net and its variants for medical image segmentation: A review of theory and applications","Ieee ","2021"
"Max-deeplab: End-to-end panoptic segmentation with mask transformers","Proceedings of the ","2021"
"Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm","arXiv preprint arXiv ","2021"
"On the integration of self-attention and convolution","Proceedings of the ","2022"
"Visual saliency transformer","Proceedings of the IEEE ","2021"
"Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action","Conference on Robot ","2023"
"Contextual transformer networks for visual recognition","IEEE Transactions on Pattern ","2022"
"Training compute-optimal large language models","arXiv preprint arXiv ","2022"
"Generative adversarial networks in medical image augmentation: A review","Computers in Biology ","2022"
"Rethinking and improving relative position encoding for vision transformer","Proceedings of the IEEE ","2021"
"Conversational agents in therapeutic interventions for neurodevelopmental disorders: a survey","ACM Computing Surveys","2023"
"Transforming model prediction for tracking","Proceedings of the ","2022"
"Gan inversion: A survey","IEEE Transactions on ","2022"
"Motr: End-to-end multiple-object tracking with transformer","European Conference on ","2022"
"Geometric transformer for fast and robust point cloud registration","Proceedings of the ","2022"
"Up-detr: Unsupervised pre-training for object detection with transformers"," of the IEEE/CVF conference on ","2021"
"Rethinking network design and local geometry in point cloud: A simple residual MLP framework","arXiv preprint arXiv:2202.07123","2022"
"Blended latent diffusion","ACM Transactions on Graphics ","2023"
"Bevt: Bert pretraining of video transformers","Proceedings of the ","2022"
"Large language models are human-level prompt engineers","arXiv preprint arXiv ","2022"
"Beyond english-centric multilingual machine translation","The Journal of Machine ","2021"
"Vision-language pre-training: Basics, recent advances, and future trends","Foundations and Trends ","2022"
"Transgan: Two pure transformers can make one strong gan, and that can scale up","Advances in Neural ","2021"
"What Makes Good In-Context Examples for GPT-?","arXiv preprint arXiv ","2021"
"Dualprompt: Complementary prompting for rehearsal-free continual learning"," on Computer Vision","2022"
"Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting","Proceedings of the IEEE ","2021"
"Fnet: Mixing tokens with fourier transforms","arXiv preprint arXiv ","2021"
"It's not just size that matters: Small language models are also few-shot learners","arXiv preprint arXiv:2009.07118","2020"
"Swintrack: A simple and strong baseline for transformer tracking","Advances in Neural ","2022"
"The impact of AlphaFold2 one year on","Nature methods","2022"
"Escaping the big data paradigm with compact transformers","arXiv preprint arXiv ","2021"
"Advances in neural rendering","Computer Graphics ","2022"
"Single-sequence protein structure prediction using a language model and deep learning","Nature ","2022"
"Patches are all you need?","arXiv preprint arXiv:2201.09792","2022"
"A survey of human-in-the-loop for machine learning","Future Generation Computer ","2022"
"Transformer for single image super-resolution","Proceedings of the ","2022"
"Big bird: Transformers for longer sequences","Advances in neural ","2020"
"Domain-specific language model pretraining for biomedical natural language processing","ACM Transactions on ","2021"
"Efficientformer: Vision transformers at mobilenet speed","Advances in ","2022"
"Multimae: Multi-modal multi-task masked autoencoders","European Conference on ","2022"
"How to build a cognitive map","Nature ","2022"
"Involution: Inverting the inherence of convolution for visual recognition","Proceedings of the ","2021"
"Volo: Vision outlooker for visual recognition","IEEE transactions on ","2022"
"Adabelief optimizer: Adapting stepsizes by the belief in observed gradients","Advances in neural ","2020"
"Luke: deep contextualized entity representations with entity-aware self-attention","arXiv preprint arXiv ","2020"
"Diffwave: A versatile diffusion model for audio synthesis","arXiv preprint arXiv ","2020"
"Transtrack: Multiple object tracking with transformer","arXiv preprint arXiv ","2020"
"You only look at one sequence: Rethinking transformer in vision through object detection","Advances in ","2021"
"[HTML][HTML] Ranking the effectiveness of worldwide COVID-19 government interventions","Nature human ","2020"
"Neural sparse voxel fields","Advances in Neural ","2020"
"Layoutlmv3: Pre-training for document ai with unified text and image masking","Proceedings of the 30th ACM ","2022"
"Physdiff: Physics-guided human motion diffusion model","Proceedings of the ","2023"
"Flowformer: A transformer architecture for optical flow"," on Computer Vision","2022"
"Holistic evaluation of language models","arXiv preprint arXiv ","2022"
"Mpvit: Multi-path vision transformer for dense prediction","Proceedings of the IEEE ","2022"
"Improving 3d object detection with channel-wise transformer","Proceedings of the ","2021"
" Recent advances in end-to-end automatic speech recognition","APSIPA Transactions on Signal and Information ","2022"
"Prottrans: Toward understanding the language of life through self-supervised learning","IEEE transactions on ","2021"
"wav2vec 2.0: A framework for self-supervised learning of speech representations","Advances in neural ","2020"
"Dytox: Transformers for continual learning with dynamic token expansion","Proceedings of the ","2022"
"Denoising diffusion probabilistic models","Advances in neural information ","2020"
"Transformers are rnns: Fast autoregressive transformers with linear attention"," on machine learning","2020"
"Byt5: Towards a token-free future with pre-trained byte-to-byte models","Transactions of the ","2022"
"Expanding language-image pretrained models for general video recognition"," on Computer Vision","2022"
"On the sentence embeddings from pre-trained language models","arXiv preprint arXiv:2011.05864","2020"
"Dab-detr: Dynamic anchor boxes are better queries for detr","arXiv preprint arXiv ","2022"
"A generalization of transformer networks to graphs","arXiv preprint arXiv:2012.09699","2020"
"Fourier features let networks learn high frequency functions in low dimensional domains","Advances in ","2020"
"Generative pretraining from pixels","International ","2020"
"Visual prompting via image inpainting","Advances in Neural ","2022"
"[HTML][HTML] Earthquake transformer—an attentive deep-learning model for simultaneous earthquake detection and phase picking","Nature ","2020"
"Efficient large-scale language model training on gpu clusters using megatron-lm","Proceedings of the ","2021"
"Omnivore: A single model for many visual modalities","Proceedings of the ","2022"
"Action-conditioned 3D human motion synthesis with transformer VAE","Proceedings of the IEEE ","2021"
"Unit: Multimodal multitask learning with a unified transformer","Proceedings of the IEEE/CVF International ","2021"
"Self-supervised learning: Generative or contrastive","IEEE transactions on ","2021"
"LEGAL-BERT: The muppets straight out of law school","arXiv preprint arXiv ","2020"
"TEMOS: Generating diverse human motions from textual descriptions","European Conference on Computer ","2022"
"Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images","International MICCAI ","2021"
"Pali: A jointly-scaled multilingual language-image model","arXiv preprint arXiv ","2022"
"Roformer: Enhanced transformer with rotary position embedding","arXiv preprint arXiv ","2021"
"W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training","2021 IEEE Automatic ","2021"
"Ds-transunet: Dual swin transformer u-net for medical image segmentation","IEEE Transactions on ","2022"
"Deberta: Decoding-enhanced bert with disentangled attention","arXiv preprint arXiv:2006.03654","2020"
"Transfuser: Imitation with transformer-based sensor fusion for autonomous driving"," on Pattern Analysis ","2022"
"Pure transformers are powerful graph learners","Advances in Neural ","2022"
"End-to-end object detection with transformers","European conference on ","2020"
"Walk in the cloud: Learning curves for point clouds shape analysis","Proceedings of the ","2021"
"Linformer: Self-attention with linear complexity","arXiv preprint arXiv:2006.04768","2020"
"Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things","IEEE Internet of Things Journal","2020"
"Graph neural networks for natural language processing: A survey"," and Trends® in ","2023"
"Fastspeech 2: Fast and high-quality end-to-end text to speech","arXiv preprint arXiv ","2020"
"Versatile diffusion: Text, images and variations all in one diffusion model","Proceedings of the ","2023"
"Seeing out of the box: End-to-end pre-training for vision-language representation learning","Proceedings of the ","2021"
"Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning"," of the IEEE/CVF conference on ","2021"
"Explainability in graph neural networks: A taxonomic survey","IEEE transactions on pattern ","2022"
"On-device training under 256kb memory","Advances in Neural ","2022"
"Keeping your eye on the ball: Trajectory attention in video transformers","Advances in neural ","2021"
"Conformer: Convolution-augmented transformer for speech recognition","arXiv preprint arXiv ","2020"
"Gcc: Graph contrastive coding for graph neural network pre-training","Proceedings of the 26th ","2020"
"Large language models encode clinical knowledge","arXiv preprint arXiv ","2022"
"How much can clip benefit vision-and-language tasks?","arXiv preprint arXiv ","2021"
"Learning to summarize with human feedback","Advances in ","2020"
"Zero-shot image-to-image translation","ACM SIGGRAPH 2023 ","2023"
"Towards language-free training for text-to-image generation","Proceedings of the ","2022"
"Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond","International Journal of Computer Vision","2023"
"Learning texture transformer network for image super-resolution","Proceedings of the IEEE ","2020"
"[HTML][HTML] Deep Learning applications for COVID-19","Journal of big Data","2021"
"Grand: Graph neural diffusion","International ","2021"
"History aware multimodal transformer for vision-and-language navigation","Advances in neural ","2021"
" Learning the protein language: Evolution, structure, and function","Cell systems","2021"
"Realtoxicityprompts: Evaluating neural toxic degeneration in language models","arXiv preprint arXiv ","2020"
"Distilling knowledge via knowledge review","Proceedings of the IEEE/CVF ","2021"
"Graphcodebert: Pre-training code representations with data flow","arXiv preprint arXiv ","2020"
"A survey on neural speech synthesis","arXiv preprint arXiv:2106.15561","2021"
"Efficiently teaching an effective dense retriever with balanced topic aware sampling","Proceedings of the 44th ","2021"
"Anchor detr: Query design for transformer-based detector"," of the AAAI conference on artificial ","2022"
"Mask dino: Towards a unified transformer-based framework for object detection and segmentation","Proceedings of the ","2023"
"Mhformer: Multi-hypothesis transformer for 3d human pose estimation","Proceedings of the ","2022"
"[HTML][HTML] Multimodal neurons in artificial neural networks","Distill","2021"
"Dynabench: Rethinking benchmarking in NLP","arXiv preprint arXiv ","2021"
"Machine Learning for industrial applications: A comprehensive literature review","Expert Systems with ","2021"
"Don't stop pretraining: Adapt language models to domains and tasks","arXiv preprint arXiv ","2020"
"Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation"," , France, September 27–October 1, 2021 ","2021"
"Gmflow: Learning optical flow via global matching","Proceedings of the ","2022"
"Transfg: A transformer architecture for fine-grained recognition","Proceedings of the ","2022"
"Object-centric learning with slot attention","Advances in ","2020"
"Learning to estimate hidden motions with global motion aggregation","Proceedings of the ","2021"
"Attention is not all you need: Pure attention loses rank doubly exponentially with depth"," Conference on Machine ","2021"
"[HTML][HTML] Drug discovery with explainable artificial intelligence","Nature Machine Intelligence","2020"
"Camouflaged object segmentation with distraction mining","Proceedings of the ","2021"
"Trocr: Transformer-based optical character recognition with pre-trained models","Proceedings of the ","2023"
"Attentional feature fusion","Proceedings of the ","2021"
"Longformer: The long-document transformer","arXiv preprint arXiv:2004.05150","2020"
"Transformer quality in linear time","International Conference on ","2022"
"Crypten: Secure multi-party computation meets machine learning","Advances in ","2021"
"Coderl: Mastering code generation through pretrained models and deep reinforcement learning","Advances in Neural ","2022"
"Long-tailed classification by keeping the good and removing the bad momentum causal effect","Advances in Neural ","2020"
"A transformer-based framework for multivariate time series representation learning","Proceedings of the 27th ","2021"
"3d object detection with pointformer","Proceedings of the IEEE ","2021"
"Diffusion Schrödinger bridge with applications to score-based generative modeling","Advances in Neural ","2021"
"Brain tumor segmentation based on the fusion of deep semantics and edge information in multimodal MRI","Information Fusion","2023"
"Point transformer v2: Grouped vector attention and partition-based pooling","Advances in Neural ","2022"
"Contrastive representation learning: A framework and review","Ieee Access","2020"
"Self-supervised hypergraph convolutional networks for session-based recommendation","Proceedings of the AAAI ","2021"
"Spanish pre-trained bert model and evaluation data","arXiv preprint arXiv ","2023"
"Gligen: Open-set grounded text-to-image generation","Proceedings of the ","2023"
"Colbert: Efficient and effective passage search via contextualized late interaction over bert","Proceedings of the 43rd International ACM SIGIR ","2020"
"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks","Computer Vision–ECCV ","2020"
"ProteinBERT: a universal deep-learning model of protein sequence and function","N Brandes, D Ofer, Y Peleg, N Rappoport - , 2022 - academic.oup.com","2022"
"Multi-modal transformer for video retrieval"," 28, 2020, Proceedings, Part IV 16","2020"
"Shortcut learning in deep neural networks","Nature Machine ","2020"
"Deep learning--based text classification: a comprehensive review","ACM computing ","2021"
"Multiple knowledge representation for big data artificial intelligence: framework, applications, and case studies","Frontiers of Information Technology & ","2021"
"A spatial-temporal attention-based method and a new dataset for remote sensing image change detection","Remote Sensing","2020"
"An attention-based deep learning approach for sleep stage classification with single-channel EEG"," on Neural Systems ","2021"
"[HTML][HTML] Review of image classification algorithms based on convolutional neural networks","Remote Sensing","2021"
"[HTML][HTML] ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports","European ","2023"
"Putting nerf on a diet: Semantically consistent few-shot view synthesis","Proceedings of the IEEE/CVF ","2021"
"Geometric deep learning on molecular representations","Nature Machine Intelligence","2021"
"Scale-mae: A scale-aware masked autoencoder for multiscale geospatial representation learning","Proceedings of the ","2023"
"Autoformer: Searching transformers for visual recognition","Proceedings of the IEEE ","2021"
"Wavegrad: Estimating gradients for waveform generation","arXiv preprint arXiv ","2020"
"Unsupervised cross-lingual representation learning for speech recognition","arXiv preprint arXiv ","2020"
"Pre-trained models for natural language processing: A survey","Science China ","2020"
"Recipes for building an open-domain chatbot","arXiv preprint arXiv ","2020"
"Ilvr: Conditioning method for denoising diffusion probabilistic models","arXiv preprint arXiv:2108.02938","2021"
"Exploring self-attention for image recognition"," of the IEEE/CVF conference on ","2020"
"BLEURT: Learning robust metrics for text generation","arXiv preprint arXiv:2004.04696","2020"
"Docformer: End-to-end transformer for document understanding","Proceedings of the ","2021"
"Self-supervised graph transformer on large-scale molecular data","Advances in ","2020"
"UTNet: a hybrid transformer architecture for medical image segmentation"," , France, September 27–October 1, 2021 ","2021"
"Nerf: Representing scenes as neural radiance fields for view synthesis","Communications of the ","2021"
"[HTML][HTML] Foundation models for generalist medical artificial intelligence","Nature","2023"
"Se (3)-transformers: 3d roto-translation equivariant attention networks","Advances in neural ","2020"
"Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models","IEEE transactions on ","2021"
"Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation","International ","2020"
"[HTML][HTML] Machine learning in protein structure prediction","Current opinion in chemical biology","2021"
"Musiq: Multi-scale image quality transformer","Proceedings of the ","2021"
"Language-agnostic BERT sentence embedding","arXiv preprint arXiv ","2020"
"Cris: Clip-driven referring image segmentation","Proceedings of the ","2022"
"Rethinking transformer-based set prediction for object detection","Proceedings of the IEEE ","2021"
"Pyramid r-cnn: Towards better performance and adaptability for 3d object detection","Proceedings of the ","2021"
"A primer in BERTology: What we know about how BERT works","Transactions of the Association ","2021"
"On faithfulness and factuality in abstractive summarization","arXiv preprint arXiv ","2020"
"Morel: Model-based offline reinforcement learning","Advances in neural ","2020"
"Chatting and cheating: Ensuring academic integrity in the era of ChatGPT","Innovations in Education ","2023"
"Snowflakenet: Point cloud completion by snowflake point deconvolution with skip-transformer","Proceedings of the ","2021"
"Language models are few-shot learners","Advances in neural ","2020"
"Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training","Advances in neural ","2022"
"Codebert: A pre-trained model for programming and natural languages","arXiv preprint arXiv ","2020"
"Rethinking vision transformers for mobilenet size and speed","Proceedings of the ","2023"
"Long range arena: A benchmark for efficient transformers","arXiv preprint arXiv ","2020"
"Unixcoder: Unified cross-modal pre-training for code representation","arXiv preprint arXiv ","2022"
"Muse: Text-to-image generation via masked generative transformers","arXiv preprint arXiv ","2023"
"Predator: Registration of 3d point clouds with low overlap","Proceedings of the ","2021"
"Nyströmformer: A nyström-based algorithm for approximating self-attention","Proceedings of the ","2021"
"Rstnet: Captioning with adaptive attention on visual and non-visual words","Proceedings of the ","2021"
"Vectornet: Encoding hd maps and agent dynamics from vectorized representation","Proceedings of the ","2020"
"[HTML][HTML] Catalyzing next-generation artificial intelligence through neuroai","Nature ","2023"
"Snowflake point deconvolution for point cloud completion and generation with skip-transformer"," on Pattern Analysis ","2022"
"Axial-deeplab: Stand-alone axial-attention for panoptic segmentation","European conference on ","2020"
"Graphmae: Self-supervised masked graph autoencoders","Proceedings of the 28th ","2022"
"Instances as queries","Proceedings of the ","2021"
"SemEval-2020 task 12: Multilingual offensive language identification in social media (OffensEval 2020)","arXiv preprint arXiv ","2020"
"S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization","Proceedings of the 29th ","2020"
"Retrieval-augmented generation for knowledge-intensive nlp tasks","Advances in ","2020"
"Revisiting pre-trained models for Chinese natural language processing","arXiv preprint arXiv ","2020"
"Rest: An efficient transformer for visual recognition","Advances in neural information ","2021"
"A-vit: Adaptive tokens for efficient vision transformer","Proceedings of the ","2022"
"Global tracking transformers","Proceedings of the IEEE ","2022"
"MSA transformer","International ","2021"
" Benchmarking graph neural networks","arXiv preprint arXiv ","2020"
"Multilingual denoising pre-training for neural machine translation","Transactions of the ","2020"
"Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition","Proceedings of the ","2022"
"Mpnet: Masked and permuted pre-training for language understanding","Advances in Neural ","2020"
"Motionclip: Exposing human motion generation to clip space"," on Computer Vision","2022"
"Arabert: Transformer-based model for arabic language understanding","arXiv preprint arXiv:2003.00104","2020"
"Ai choreographer: Music conditioned 3d dance generation with aist++","Proceedings of the IEEE ","2021"
"Uniformer: Unifying convolution and self-attention for visual recognition"," on Pattern Analysis ","2023"
"Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation","Proceedings of the ","2020"
"Summeval: Re-evaluating summarization evaluation","Transactions of the ","2021"
"Mesh graphormer","Proceedings of the IEEE/CVF ","2021"
"Semantic communications for future internet: Fundamentals, applications, and challenges"," Surveys & Tutorials","2022"
"Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers","Advances in Neural ","2020"
"BERTimbau: pretrained BERT models for Brazilian Portuguese"," 2020, Rio Grande, Brazil, October 20–23 ","2020"
"Fast convergence of detr with spatially modulated co-attention","Proceedings of the IEEE ","2021"
"Patching open-vocabulary models by interpolating weights","Advances in ","2022"
"Machine learning and AI in marketing–Connecting computing power to human insights","International Journal of Research in Marketing","2020"
"X-linear attention networks for image captioning"," of the IEEE/CVF conference on ","2020"
"Deep learning enabled semantic communication systems","IEEE Transactions on Signal ","2021"
"A survey on knowledge graph-based recommender systems"," on Knowledge and ","2020"
"End-to-end generative pretraining for multimodal video captioning","Proceedings of the ","2022"
"Emotion recognition from speech using wav2vec 2.0 embeddings","arXiv preprint arXiv:2104.03502","2021"
"Exploring and distilling posterior and prior knowledge for radiology report generation","Proceedings of the IEEE ","2021"
"Sequential recommendation with graph neural networks","Proceedings of the 44th ","2021"
"Large-scale adversarial training for vision-and-language representation learning","Advances in Neural ","2020"
"NTIRE 2023 image shadow removal challenge report","Proceedings of the ","2023"
"Generalizing face forgery detection with high-frequency features","Proceedings of the IEEE ","2021"
"Stmtrack: Template-free visual tracking with space-time memory networks"," of the IEEE/CVF conference on ","2021"
"Laplace redux-effortless bayesian deep learning","Advances in ","2021"
"Clear: Contrastive learning for sentence representation","arXiv preprint arXiv ","2020"
"Aiatrack: Attention in attention for transformer visual tracking","European Conference on ","2022"
"Multimodal motion prediction with stacked transformers","Proceedings of the ","2021"
"Cotr: Correspondence transformer for matching across images","Proceedings of the ","2021"
"[Ã¥][B] Pretrained transformers for text ranking: Bert and beyond","J Lin, R Nogueira, A Yates - 2022 - books.google.com","2022"
"Generative adversarial transformers","International conference on machine ","2021"
"Vision transformers for remote sensing image classification","Remote Sensing","2021"
"BERTweet: A pre-trained language model for English Tweets","arXiv preprint arXiv:2005.10200","2020"
"Pegasus: Pre-training with extracted gap-sentences for abstractive summarization"," Conference on Machine ","2020"
"The evolution, evolvability and engineering of gene regulatory DNA","Nature","2022"
"Heterogeneous graph transformer","Proceedings of the web conference 2020","2020"
"Galactica: A large language model for science","arXiv preprint arXiv ","2022"
"Visual transformers: Token-based image representation and processing for computer vision","arXiv preprint arXiv ","2020"
"Align and prompt: Video-and-language pre-training with entity prompts","Proceedings of the IEEE ","2022"
"Visual chatgpt: Talking, drawing and editing with visual foundation models","arXiv preprint arXiv ","2023"
"Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models","arXiv preprint arXiv:2106.10199","2021"
"E-CRF: Embedded Conditional Random Field for Boundary-caused Class Weights Confusion in Semantic Segmentation","arXiv preprint arXiv:2112.07106","2021"
"Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation","International Conference on ","2023"
"A generalist framework for panoptic segmentation of images and videos","Proceedings of the ","2023"
"Explainable deep learning models in medical image analysis","Journal of imaging","2020"
"[HTML][HTML] Shifting machine learning for healthcare from development to deployment and from models to data","Nature Biomedical Engineering","2022"
"Ssast: Self-supervised audio spectrogram transformer"," of the AAAI Conference on Artificial ","2022"
"Normalizing flows for probabilistic modeling and inference","The Journal of Machine ","2021"
"A simple language model for task-oriented dialogue","Advances in Neural ","2020"
"[Ã¥][B] The principles of deep learning theory","DA Roberts, S Yaida, B Hanin - 2022 - cambridge.org","2022"
"Superglue: Learning feature matching with graph neural networks","Proceedings of the ","2020"
"Meshed-memory transformer for image captioning","Proceedings of the ","2020"
"3d-aware image synthesis via learning structural and textural representations","Proceedings of the ","2022"
"Jcs: An explainable covid-19 diagnosis system by joint classification and segmentation"," on Image Processing","2021"
"Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling","Proceedings of the IEEE ","2020"
"Quantifying attention flow in transformers","arXiv preprint arXiv:2005.00928","2020"
"Deep residual learning in spiking neural networks","Advances in Neural ","2021"
"Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition","Proceedings of the ","2021"
"Deepfakes and beyond: A survey of face manipulation and fake detection","Information ","2020"
"Graph learning: A survey","IEEE Transactions on ","2021"
"Ogb-lsc: A large-scale challenge for machine learning on graphs","arXiv preprint arXiv ","2021"
"Membership inference attacks on machine learning: A survey","ACM Computing Surveys ","2022"
"TopFormer: Token pyramid transformer for mobile semantic segmentation","Proceedings of the ","2022"
"Multi-agent reinforcement learning: A selective overview of theories and algorithms","Handbook of reinforcement learning and ","2021"
"Vibe: Video inference for human body pose and shape estimation","Proceedings of the IEEE ","2020"
"Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks","IEEE transactions on pattern analysis and ","2021"
"Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding","Proceedings of The Web Conference ","2020"
"Wayformer: Motion forecasting via simple & efficient attention networks"," on Robotics and ","2023"
"Cross-lingual language model pretraining","Advances in neural information ","2019"
"[HTML][HTML] Temporal fusion transformers for interpretable multi-horizon time series forecasting","International Journal of Forecasting","2021"
" Language models of protein sequences at the scale of evolution enable accurate structure prediction","BioRxiv","2022"
"Swformer: Sparse window transformer for 3d object detection in point clouds"," on Computer Vision","2022"
"Retrieval augmentation reduces hallucination in conversation","arXiv preprint arXiv ","2021"
"How much knowledge can you pack into the parameters of a language model?","arXiv preprint arXiv:2002.08910","2020"
"Audiolm: a language modeling approach to audio generation"," on Audio, Speech ","2023"
"Transformer models for text-based emotion detection: a review of BERT-based approaches","Artificial Intelligence ","2021"
"Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows","Proceedings of the IEEE ","2022"
"Attention is all you need in speech separation","ICASSP 2021-2021 ","2021"
"An introduction to deep learning in natural language processing: Models, techniques, and tools","Neurocomputing","2022"
"Time series data augmentation for deep learning: A survey","arXiv preprint arXiv ","2020"
"How can we know what language models know?","Transactions of the Association for ","2020"
"Self-supervised multi-channel hypergraph convolutional network for social recommendation","Proceedings of the web ","2021"
"A survey of controllable text generation using transformer-based pre-trained language models","ACM Computing Surveys","2023"
"Efficient self-supervised vision transformers for representation learning","arXiv preprint arXiv ","2021"
"Dialogpt: Large-scale generative pre-training for conversational response generation","arXiv preprint arXiv ","2019"
"Pushing the limits of semi-supervised learning for automatic speech recognition","arXiv preprint arXiv ","2020"
"Gman: A graph multi-attention network for traffic prediction","Proceedings of the AAAI conference on ","2020"
"Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale","International ","2022"
"Declutr: Deep contrastive learning for unsupervised textual representations","arXiv preprint arXiv:2006.03659","2020"
"Transformers: State-of-the-art natural language processing","Proceedings of the ","2020"
"[HTML][HTML] Combined scaling for zero-shot transfer learning","Neurocomputing","2023"
"Speaker recognition based on deep learning: An overview","Neural Networks","2021"
"Transvg: End-to-end visual grounding with transformers","Proceedings of the IEEE ","2021"
"Social interactions for autonomous driving: A review and perspectives","Foundations and Trends ","2022"
"Adapterhub: A framework for adapting transformers","arXiv preprint arXiv ","2020"
"Huggingface's transformers: State-of-the-art natural language processing","arXiv preprint arXiv ","2019"
"Transformer memory as a differentiable search index","Advances in ","2022"
"COVIDSenti: A large-scale benchmark Twitter data set for COVID-19 sentiment analysis","IEEE transactions on ","2021"
"Virtex: Learning visual representations from textual annotations"," of the IEEE/CVF conference on ","2021"
"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","arXiv preprint arXiv:1910.01108","2019"
"[HTML][HTML] The forward physics facility at the high-luminosity LHC","Journal of Physics G ","2023"
"Temporal graph networks for deep learning on dynamic graphs","arXiv preprint arXiv ","2020"
"Hyper-parameter optimization: A review of algorithms and applications","arXiv preprint arXiv:2003.05689","2020"
"VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation","arXiv preprint arXiv ","2021"
"Actbert: Learning global-local video-text representations","Proceedings of the IEEE/CVF conference ","2020"
"A survey of the state of explainable AI for natural language processing","arXiv preprint arXiv ","2020"
"Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks","International Conference on ","2021"
"Reformer: The efficient transformer","arXiv preprint arXiv:2001.04451","2020"
"Simpler is better: Few-shot semantic segmentation with classifier weight transformer","Proceedings of the ","2021"
"Mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction","Proceedings of the ","2022"
"Electra: Pre-training text encoders as discriminators rather than generators","arXiv preprint arXiv:2003.10555","2020"
"Albert: A lite bert for self-supervised learning of language representations","arXiv preprint arXiv ","2019"
"Deep image deblurring: A survey","International Journal of ","2022"
"Understanding and mitigating gradient flow pathologies in physics-informed neural networks","SIAM Journal on Scientific Computing","2021"
"Joint feature learning and relation modeling for tracking: A one-stream framework","European Conference on ","2022"
"Anticipative video transformer","Proceedings of the IEEE/CVF ","2021"
"Learning lane graph representations for motion forecasting","Computer Vision–ECCV ","2020"
"Extractive summarization as text matching","arXiv preprint arXiv ","2020"
"Styleswin: Transformer-based gan for high-resolution image generation","Proceedings of the ","2022"
"Object-contextual representations for semantic segmentation","Computer Vision–ECCV 2020: 16th European ","2020"
"Heterofl: Computation and communication efficient federated learning for heterogeneous clients","arXiv preprint arXiv:2010.01264","2020"
"Tip-adapter: Training-free clip-adapter for better vision-language modeling","arXiv preprint arXiv ","2021"
"Tinybert: Distilling bert for natural language understanding","arXiv preprint arXiv ","2019"
"The lottery ticket hypothesis for pre-trained bert networks","Advances in neural ","2020"
"Semi-supervised city-wide parking availability prediction via hierarchical recurrent graph neural network","IEEE Transactions on ","2020"
"Multi-class token transformer for weakly supervised semantic segmentation","Proceedings of the ","2022"
"Template-based named entity recognition using BART","arXiv preprint arXiv:2106.01760","2021"
"A survey on evaluation of large language models","arXiv preprint arXiv ","2023"
"Codegen: An open large language model for code with multi-turn program synthesis","arXiv preprint arXiv ","2022"
"Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition","IEEE Journal of ","2022"
"Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram","ICASSP 2020-2020 IEEE ","2020"
"Multiscale Vision Transformers.","ICCV","2021"
"Dive into deep learning","arXiv preprint arXiv:2106.11342","2021"
"Semantic communication systems for speech transmission","IEEE Journal on Selected Areas in ","2021"
"A survey of android malware detection with deep neural models","ACM Computing Surveys ","2020"
"Uniter: Universal image-text representation learning","European conference on ","2020"
"From show to tell: A survey on deep learning-based image captioning","IEEE transactions on ","2022"
"TaBERT: Pretraining for joint understanding of textual and tabular data","arXiv preprint arXiv:2005.08314","2020"
"Masked world models for visual control"," on Robot Learning","2023"
"Sentence-bert: Sentence embeddings using siamese bert-networks","arXiv preprint arXiv:1908.10084","2019"
"Few-shot incremental learning with continually evolved classifiers","Proceedings of the ","2021"
"[HTML][HTML] A survey of deep meta-learning","Artificial Intelligence Review","2021"
"Multimodal learning with transformers: A survey","IEEE Transactions on Pattern Analysis ","2023"
"Towards large-scale small object detection: Survey and benchmarks"," on Pattern Analysis ","2023"
"Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization","Proceedings of the ","2022"
"Deep double descent: Where bigger models and more data hurt","Journal of Statistical ","2021"
"Surface representation for point clouds"," of the IEEE/CVF Conference on ","2022"
"Frozen clip models are efficient video learners"," on Computer Vision","2022"
"Deep learning-based human pose estimation: A survey","ACM Computing ","2023"
"Lxmert: Learning cross-modality encoder representations from transformers","arXiv preprint arXiv:1908.07490","2019"
"[HTML][HTML] Deep learning for cardiac image segmentation: a review","Frontiers in ","2020"
"Sign language transformers: Joint end-to-end sign language recognition and translation","Proceedings of the ","2020"
"Focal attention for long-range interactions in vision transformers","Advances in Neural ","2021"
"Text summarization with pretrained encoders","arXiv preprint arXiv:1908.08345","2019"
"Vl-bert: Pre-training of generic visual-linguistic representations","arXiv preprint arXiv ","2019"
"Tokenlearner: Adaptive space-time tokenization for videos","Advances in ","2021"
"Efficiently modeling long sequences with structured state spaces","arXiv preprint arXiv:2111.00396","2021"
"Ctrl: A conditional transformer language model for controllable generation","arXiv preprint arXiv ","2019"
"Unified vision-language pre-training for image captioning and vqa","Proceedings of the AAAI ","2020"
"Resunet++: An advanced architecture for medical image segmentation"," on multimedia (ISM)","2019"
"Global context enhanced graph neural networks for session-based recommendation","Proceedings of the 43rd ","2020"
" BERTERS: Multimodal Representation Learning for Expert Recommendation System with Transformer","arXiv preprint arXiv ","2020"
"Hero: Hierarchical encoder for video+ language omni-representation pre-training","arXiv preprint arXiv ","2020"
"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks","Advances in neural ","2019"
"Movinets: Mobile video networks for efficient video recognition","Proceedings of the ","2021"
"Autosdf: Shape priors for 3d completion, reconstruction and generation","Proceedings of the ","2022"
"On the variance of the adaptive learning rate and beyond","arXiv preprint arXiv ","2019"
"Deep graph library: A graph-centric, highly-performant package for graph neural networks","arXiv preprint arXiv ","2019"
"A survey on instance segmentation: state of the art","International journal of multimedia information ","2020"
" Pretrained transformers as universal computation engines","arXiv preprint arXiv:2103.05247","2021"
"DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome","Bioinformatics","2021"
"Inductive biases for deep learning of higher-level cognition","Proceedings of the Royal Society A","2022"
"Roberta: A robustly optimized bert pretraining approach","arXiv preprint arXiv ","2019"
"Representing long-range context for graph neural networks with global attention","Advances in ","2021"
"No fear of heterogeneity: Classifier calibration for federated learning with non-iid data","Advances in Neural ","2021"
"Spatio-temporal graph transformer networks for pedestrian trajectory prediction"," , Glasgow, UK, August 23–28, 2020 ","2020"
"Reinforcement learning for combinatorial optimization: A survey","Computers & Operations ","2021"
"On layer normalization in the transformer architecture","International ","2020"
"Associating objects with transformers for video object segmentation","Advances in Neural Information ","2021"
"Attention on attention for image captioning","Proceedings of the IEEE ","2019"
"Persistent anti-muslim bias in large language models","Proceedings of the 2021 AAAI/ACM ","2021"
"[HTML][HTML] Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction","NPJ digital medicine","2021"
"Tokenpose: Learning keypoint tokens for human pose estimation","Proceedings of the ","2021"
"Emergent tool use from multi-agent autocurricula","arXiv preprint arXiv ","2019"
"A comparative study on transformer vs rnn in speech applications","2019 IEEE Automatic ","2019"
"Explainable deep learning: A field guide for the uninitiated","Journal of Artificial Intelligence ","2022"
"A simple framework for open-vocabulary segmentation and detection","Proceedings of the ","2023"
"K-bert: Enabling language representation with knowledge graph","Proceedings of the ","2020"
"Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures","Advances in Neural ","2022"
"Neural collaborative filtering vs. matrix factorization revisited","Proceedings of the 14th ","2020"
"Grandmaster level in StarCraft II using multi-agent reinforcement learning","Nature","2019"
"[HTML][HTML] A literature survey of recent advances in chatbots","Information","2022"
"Knowledge enhanced contextual word representations","arXiv preprint arXiv ","2019"
"Hift: Hierarchical feature transformer for aerial tracking","Proceedings of the IEEE/CVF ","2021"
"nnformer: Interleaved transformer for volumetric segmentation","arXiv preprint arXiv ","2021"
"12-in-1: Multi-task vision and language representation learning","Proceedings of the ","2020"
"Glow-tts: A generative flow for text-to-speech via monotonic alignment search","Advances in Neural ","2020"
"Regtr: End-to-end point cloud correspondences with transformers","Proceedings of the IEEE/CVF conference ","2022"
"Learning knowledge graph embedding with heterogeneous relation attention networks","IEEE Transactions on ","2021"
"Jukebox: A generative model for music","arXiv preprint arXiv ","2020"
"A survey on generative diffusion model","arXiv preprint arXiv ","2022"
"What can transformers learn in-context? a case study of simple function classes","Advances in Neural ","2022"
"[HTML][HTML] Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals","Nature ","2020"
"Jkt: A joint graph convolutional network based deep knowledge tracing","Information Sciences","2021"
"[HTML][HTML] A large language model for electronic health records","NPJ Digital ","2022"
"ERNIE-ViLG 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts","Proceedings of the ","2023"
"Masked label prediction: Unified message passing model for semi-supervised classification","arXiv preprint arXiv ","2020"
"Xlnet: Generalized autoregressive pretraining for language understanding","Advances in neural ","2019"
"Qplex: Duplex dueling multi-agent q-learning","arXiv preprint arXiv:2008.01062","2020"
"All tokens matter: Token labeling for training better vision transformers","Advances in neural ","2021"
"Minivit: Compressing vision transformers with weight multiplexing","Proceedings of the ","2022"
"Disentangled graph collaborative filtering","Proceedings of the 43rd ","2020"
"Rethinking space-time networks with improved memory coverage for efficient video object segmentation","Advances in Neural ","2021"
"Asymmetric non-local neural networks for semantic segmentation","Proceedings of the IEEE ","2019"
"FLAT: Chinese NER using flat-lattice transformer","arXiv preprint arXiv:2004.11795","2020"
"Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics","arXiv preprint arXiv:2104.13346","2021"
"Software vulnerability detection using deep neural networks: a survey","Proceedings of the ","2020"
"Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting","Advances in neural ","2019"
"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs","Proceedings of the AAAI ","2021"
"Text2Event: Controllable sequence-to-structure generation for end-to-end event extraction","arXiv preprint arXiv ","2021"
"Exploring the limits of transfer learning with a unified text-to-text transformer","The Journal of Machine ","2020"
"MCVD-masked conditional video diffusion for prediction, generation, and interpolation","Advances in Neural ","2022"
"Chasing sparsity in vision transformers: An end-to-end exploration","Advances in Neural ","2021"
"An explanation of in-context learning as implicit bayesian inference","arXiv preprint arXiv:2111.02080","2021"
"vq-wav2vec: Self-supervised learning of discrete speech representations","arXiv preprint arXiv:1910.05453","2019"
"Class-incremental learning by knowledge distillation with adaptive feature consolidation"," of the IEEE/CVF conference on ","2022"
"Ernie 2.0: A continual pre-training framework for language understanding","Proceedings of the AAAI ","2020"
"Energy and policy considerations for deep learning in NLP","arXiv preprint arXiv:1906.02243","2019"
"Pretrained transformers improve out-of-distribution robustness","arXiv preprint arXiv ","2020"
"On the cross-lingual transferability of monolingual representations","arXiv preprint arXiv:1910.11856","2019"
"Pose recognition with cascade transformers","Proceedings of the ","2021"
"AdapterFusion: Non-destructive task composition for transfer learning","arXiv preprint arXiv ","2020"
"Can language models learn from explanations in context?","arXiv preprint arXiv ","2022"
"Masked discrimination for self-supervised learning on point clouds","European Conference on Computer Vision","2022"
"Structure-aware transformer for graph representation learning"," Conference on Machine ","2022"
"SCF-Net: Learning spatial contextual features for large-scale point cloud segmentation","Proceedings of the ","2021"
"Learning affinity from attention: End-to-end weakly-supervised semantic segmentation with transformers"," of the IEEE/CVF Conference on ","2022"
"Deep equilibrium models","Advances in Neural Information ","2019"
"Lookahead optimizer: k steps forward, 1 step back","Advances in neural ","2019"
"Coco-lm: Correcting and contrasting text sequences for language model pretraining","Advances in Neural ","2021"
"When does label smoothing help?","Advances in neural ","2019"
"Plug and play language models: A simple approach to controlled text generation","arXiv preprint arXiv ","2019"
"In-place scene labelling and understanding with implicit scene representation","Proceedings of the ","2021"
"Findings of the 2019 conference on machine translation (WMT19)","L Barrault, O Bojar, MR Costa-Jussa, C Federmann - 2019 - zora.uzh.ch","2019"
"Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss","ICASSP 2020-2020 ","2020"
"Language models as knowledge bases?","arXiv preprint arXiv ","2019"
"Griddehazenet: Attention-based multi-scale network for image dehazing","Proceedings of the IEEE/CVF ","2019"
"Shunted self-attention via multi-scale token aggregation","Proceedings of the IEEE ","2022"
"A unified generative framework for various NER subtasks","arXiv preprint arXiv ","2021"
"What does bert look at? an analysis of bert's attention","arXiv preprint arXiv ","2019"
"General multi-label image classification with transformers","Proceedings of the ","2021"
"Violet: End-to-end video-language transformers with masked visual-token modeling","arXiv preprint arXiv ","2021"
"KEPLER: A unified model for knowledge embedding and pre-trained language representation","Transactions of the ","2021"
"Towards grand unification of object tracking","European Conference on ","2022"
"Csdi: Conditional score-based diffusion models for probabilistic time series imputation","Advances in Neural ","2021"
"Neural graph collaborative filtering","Proceedings of the 42nd ","2019"
"Hopfield networks is all you need","arXiv preprint arXiv ","2020"
"Dual-level collaborative transformer for image captioning","Proceedings of the ","2021"
"Srdiff: Single image super-resolution with diffusion probabilistic models","Neurocomputing","2022"
"Transformer-based multimodal information fusion for facial expression analysis","Proceedings of the ","2022"
"Unilmv2: Pseudo-masked language models for unified language model pre-training","International ","2020"
"Transweather: Transformer-based restoration of images degraded by adverse weather conditions","Proceedings of the IEEE ","2022"
"K-adapter: Infusing knowledge into pre-trained models with adapters","arXiv preprint arXiv ","2020"
"[HTML][HTML] SpookyNet: Learning force fields with electronic degrees of freedom and nonlocal effects","Nature ","2021"
"Deep learning for monocular depth estimation: A review","Neurocomputing","2021"
"Multivariate time-series anomaly detection via graph attention network"," Conference on Data ","2020"
"Evaluating the factual consistency of abstractive text summarization","arXiv preprint arXiv ","2019"
"Polarformer: Multi-camera 3d object detection with polar transformer","Proceedings of the ","2023"
"Interactive language: Talking to robots in real time","IEEE Robotics and ","2023"
"Tip-adapter: Training-free adaption of clip for few-shot classification"," on Computer Vision","2022"
"Applications of deep learning in stock market prediction: recent progress","Expert Systems with Applications","2021"
"On the relationship between self-attention and convolutional layers","arXiv preprint arXiv:1911.03584","2019"
"Generalized decoding for pixel, image, and language","Proceedings of the ","2023"
"What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models","Transactions of the Association for Computational ","2020"
"Kgat: Knowledge graph attention network for recommendation","Proceedings of the 25th ACM ","2019"
"Bertscore: Evaluating text generation with bert","arXiv preprint arXiv ","2019"
"Transfer: Learning relation-aware facial expression representations with transformers","Proceedings of the IEEE/CVF ","2021"
"Unsupervised data augmentation for consistency training","Advances in neural ","2020"
"ProGen2: exploring the boundaries of protein language models","Cell Systems","2022"
"Editing conditional radiance fields","Proceedings of the ","2021"
"UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery","ISPRS Journal of ","2022"
"Text and code embeddings by contrastive pre-training","arXiv preprint arXiv ","2022"
"Pixel-bert: Aligning image pixels with text by deep multi-modal transformers","arXiv preprint arXiv:2004.00849","2020"
"ERNIE: Enhanced language representation with informative entities","arXiv preprint arXiv ","2019"
"Unified language model pre-training for natural language understanding and generation","Advances in neural ","2019"
"The curious case of neural text degeneration","arXiv preprint arXiv ","2019"
"BERT rediscovers the classical NLP pipeline","arXiv preprint arXiv:1905.05950","2019"
"Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors","Proceedings of the ","2021"
"Deep hierarchical semantic segmentation","Proceedings of the IEEE ","2022"
"Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training","arXiv preprint arXiv ","2021"
"Iterative deep graph learning for graph neural networks: Better and robust node embeddings","Advances in neural information ","2020"
"Gcnet: Non-local networks meet squeeze-excitation networks and beyond","Proceedings of the IEEE ","2019"
"[HTML][HTML] A high-performance speech neuroprosthesis","Nature","2023"
"fairseq: A fast, extensible toolkit for sequence modeling","arXiv preprint arXiv ","2019"
"Zero: Memory optimizations toward training trillion parameter models"," Conference for High ","2020"
"Pre-training with whole word masking for chinese bert","IEEE/ACM Transactions on ","2021"
"Mls: A large-scale multilingual dataset for speech research","arXiv preprint arXiv ","2020"
"SciBERT: A pretrained language model for scientific text","arXiv preprint arXiv:1903.10676","2019"
"Finbert: Financial sentiment analysis with pre-trained language models","arXiv preprint arXiv:1908.10063","2019"
"Clip2video: Mastering video-text retrieval via image clip","arXiv preprint arXiv:2106.11097","2021"
"Deep anomaly detection for time-series data in industrial IoT: A communication-efficient on-device federated learning approach","IEEE Internet of ","2020"
"DASNet: Dual attentive fully convolutional Siamese networks for change detection in high-resolution satellite images","IEEE Journal of ","2020"
"I-bert: Integer-only bert quantization"," on machine learning","2021"
"Abd-net: Attentive but diverse person re-identification","Proceedings of the ","2019"
"ERASER: A benchmark to evaluate rationalized NLP models","arXiv preprint arXiv ","2019"
"Analog bits: Generating discrete data using diffusion models with self-conditioning","arXiv preprint arXiv:2208.04202","2022"
"GFlowNet-EM for learning compositional latent variable models","International ","2023"
"Expectation-maximization attention networks for semantic segmentation","Proceedings of the ","2019"
"Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training","arXiv preprint arXiv ","2020"
"Mining cross-image semantics for weakly supervised semantic segmentation"," , Glasgow, UK, August 23–28, 2020 ","2020"
"[HTML][HTML] Transformers in medical image analysis","Intelligent ","2023"
"Optimizing dense retrieval model training with hard negatives","Proceedings of the 44th ","2021"
"Lost in the middle: How language models use long contexts","arXiv preprint arXiv ","2023"
"Graph convolutional networks for temporal action localization","Proceedings of the ","2019"
"Lavt: Language-aware vision transformer for referring image segmentation","Proceedings of the ","2022"
"Masked visual pre-training for motor control","arXiv preprint arXiv:2203.06173","2022"
"Tinyvit: Fast pretraining distillation for small vision transformers","European Conference on ","2022"
"Rpm-net: Robust point matching using learned features","Proceedings of the IEEE/CVF conference ","2020"
"Mass: Masked sequence to sequence pre-training for language generation","arXiv preprint arXiv:1905.02450","2019"
"Oneformer: One transformer to rule universal image segmentation","Proceedings of the ","2023"
"Experience grounds language","arXiv preprint arXiv ","2020"
"Savi++: Towards end-to-end object-centric learning from real-world videos","Advances in ","2022"
" Language models are unsupervised multitask learners","OpenAI ","2019"
"Flaubert: Unsupervised language model pre-training for french","arXiv preprint arXiv ","2019"
"BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer","Proceedings of the 28th ","2019"
"[HTML][HTML] Brains and algorithms partially converge in natural language processing","Communications biology","2022"
"Align your latents: High-resolution video synthesis with latent diffusion models","Proceedings of the ","2023"
"Support-set bottlenecks for video-text representation learning","arXiv preprint arXiv ","2020"
"Identifying facemask-wearing condition using image super-resolution with classification network to prevent COVID-19","Sensors","2020"
"Vision-language transformer and query generation for referring segmentation","Proceedings of the IEEE ","2021"
"Green ai","Communications of the ACM","2020"
"Fastspeech: Fast, robust and controllable text to speech","Advances in neural ","2019"
"Videobert: A joint model for video and language representation learning","Proceedings of the ","2019"
"Disentangled non-local neural networks","Computer Vision–ECCV ","2020"
"Bipartite graph network with adaptive message passing for unbiased scene graph generation","Proceedings of the IEEE/CVF ","2021"
"Effective energy consumption forecasting using empirical wavelet transform and long short-term memory","Energy","2022"
"Crossformer++: A versatile vision transformer hinging on cross-scale attention","arXiv preprint arXiv ","2023"
"Efficient self-supervised learning with contextualized target representations for vision, speech and language"," Conference on Machine ","2023"
"Episodic transformer for vision-and-language navigation","Proceedings of the IEEE ","2021"
"How to fine-tune bert for text classification?"," 18th China National Conference, CCL 2019 ","2019"
"End-to-end human object interaction detection with hoi transformer","Proceedings of the ","2021"
"Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned","arXiv preprint arXiv ","2019"
"Large language models can be strong differentially private learners","arXiv preprint arXiv:2110.05679","2021"
"A federated learning system with enhanced feature extraction for human activity recognition","Knowledge-Based Systems","2021"
"[HTML][HTML] Multimodal transformer for unaligned multimodal language sequences","Proceedings of the ","2019"
"BioBERT: a pre-trained biomedical language representation model for biomedical text mining","J Lee, W Yoon, S Kim, D Kim, S Kim, CH So - , 2020 - academic.oup.com","2020"
"Time interval aware self-attention for sequential recommendation"," of the 13th international conference on web ","2020"
"Videogpt: Video generation using vq-vae and transformers","arXiv preprint arXiv:2104.10157","2021"
"Graph neural networks: foundation, frontiers and applications"," of the 28th ACM SIGKDD Conference ","2022"
"Utilizing graph machine learning within drug discovery and development","Briefings in ","2021"
"Cure: Code-aware neural machine translation for automatic program repair","2021 IEEE/ACM 43rd International ","2021"
"Memory-augmented dense predictive coding for video representation learning","European conference on computer vision","2020"
"Human motion diffusion model","arXiv preprint arXiv ","2022"
"Edvr: Video restoration with enhanced deformable convolutional networks","Proceedings of the ","2019"
"Tranad: Deep transformer networks for anomaly detection in multivariate time series data","arXiv preprint arXiv:2201.07284","2022"
"Probabilistic two-stage detection","arXiv preprint arXiv:2103.07461","2021"
"Avatarclip: Zero-shot text-driven generation and animation of 3d avatars","arXiv preprint arXiv ","2022"
"Multiple object tracking with correlation learning","Proceedings of the IEEE ","2021"
"Retrieving and reading: A comprehensive survey on open-domain question answering","arXiv preprint arXiv ","2021"
"Vision transformers for single image dehazing","IEEE Transactions on Image ","2023"
"Spatial-spectral transformer for hyperspectral image classification","Remote Sensing","2021"
"Efficient training of visual transformers with small datasets","Advances in Neural ","2021"
"Heterogeneous graph attention network","The world wide web ","2019"
"Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis","Proceedings of the AAAI conference on ","2021"
"Parameter-efficient transfer learning for NLP","International ","2019"
"Spanbert: Improving pre-training by representing and predicting spans","Transactions of the ","2020"
"Large-scale long-tailed recognition in an open world","Proceedings of the ","2019"
"Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset","Proceedings of the IEEE/CVF ","2021"
"Interventional few-shot learning","Advances in neural ","2020"
"Learning to dispatch for job shop scheduling via deep reinforcement learning","Advances in Neural ","2020"
"Ernie: Enhanced representation through knowledge integration","arXiv preprint arXiv ","2019"
"Few-shot object detection with fully cross-transformer","Proceedings of the ","2022"
"Attention is not explanation","arXiv preprint arXiv:1902.10186","2019"
"Are NLP models really able to solve simple math word problems?","arXiv preprint arXiv:2103.07191","2021"
"Q-bert: Hessian based ultra low precision quantization of bert","Proceedings of the ","2020"
"Are sixteen heads really better than one?","Advances in neural ","2019"
"A transformer-based approach for source code summarization","arXiv preprint arXiv ","2020"
"Unsupervised cross-lingual representation learning at scale","arXiv preprint arXiv ","2019"
"Incremental transformer structure enhanced image inpainting with masking positional encoding"," of the IEEE/CVF Conference on ","2022"
"Similarity of neural network representations revisited"," conference on machine ","2019"
"Machine learning for high-entropy alloys: Progress, challenges and opportunities","Progress in Materials Science","2023"
"3d infomax improves gnns for molecular property prediction","International ","2022"
"Flexible diffusion modeling of long videos","Advances in ","2022"
"Uncertainty-guided transformer reasoning for camouflaged object detection","Proceedings of the ","2021"
"Specter: Document-level representation learning using citation-informed transformers","arXiv preprint arXiv ","2020"
"Cross-lingual language model pretraining","arXiv preprint arXiv:1901.07291","2019"
"[HTML][HTML] Graph neural networks: A review of methods and applications","AI open","2020"
" Graph contextualized self-attention network for session-based recommendation.","IJCAI","2019"
"Misa: Modality-invariant and-specific representations for multimodal sentiment analysis","Proceedings of the 28th ACM ","2020"
"Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism","Journal of medicinal ","2019"
"Movement pruning: Adaptive sparsity by fine-tuning","Advances in Neural Information ","2020"
"Deep modular co-attention networks for visual question answering","Proceedings of the IEEE ","2019"
"Anomaly transformer: Time series anomaly detection with association discrepancy","arXiv preprint arXiv:2110.02642","2021"
"Q8bert: Quantized 8bit bert","2019 Fifth Workshop on ","2019"
"Generalized category discovery","Proceedings of the ","2022"
"Visformer: The vision-friendly transformer","Proceedings of the ","2021"
"Detrs with hybrid matching","Proceedings of the ","2023"
"Is attention interpretable?","arXiv preprint arXiv:1906.03731","2019"
"Are we done with imagenet?","arXiv preprint arXiv ","2020"
"On generative spoken language modeling from raw audio","Transactions of the ","2021"
"Fast point transformer","Proceedings of the IEEE ","2022"
"Time-series representation learning via temporal and contextual contrasting","arXiv preprint arXiv ","2021"
"Nerv: Neural representations for videos","Advances in Neural ","2021"
"Learning and evaluating contextual embedding of source code"," on machine learning","2020"
"Transmorph: Transformer for unsupervised medical image registration","Medical image analysis","2022"
"Skeleton-based action recognition with multi-stream adaptive graph convolutional networks","IEEE Transactions on Image ","2020"
"BioSeq-BLM: a platform for analyzing DNA, RNA and protein sequences based on biological language models","Nucleic acids research","2021"
