"Title","Journal","Year"
"ChatGPT for good? On opportunities and challenges of large language models for education","Learning and individualÂ ","2023"
"[HTML][HTML] Attention mechanisms in computer vision: A survey","Computational visualÂ ","2022"
"High-resolution image synthesis with latent diffusion models","Proceedings of theÂ ","2022"
"Robust speech recognition via large-scale weak supervision","InternationalÂ ","2023"
"A convnet for the 2020s","Proceedings of theÂ ","2022"
"Palm: Scaling language modeling with pathways","arXiv preprint arXivÂ ","2022"
"Flamingo: a visual language model for few-shot learning","Advances inÂ ","2022"
" Instant neural graphics primitives with a multiresolution hash encoding","ACM Transactions on GraphicsÂ ","2022"
"Glide: Towards photorealistic image generation and editing with text-guided diffusion models","arXiv preprint arXivÂ ","2021"
" Scaling autoregressive models for content-rich text-to-image generation","arXiv preprint arXivÂ ","2022"
"Large language models are zero-shot reasoners","Advances in neuralÂ ","2022"
"Masked-attention mask transformer for universal image segmentation","Proceedings of theÂ ","2022"
"Data2vec: A general framework for self-supervised learning in speech, vision and language","Â on Machine Learning","2022"
"Bloom: A 176b-parameter open-access multilingual language model","arXiv preprint arXivÂ ","2022"
"Laion-5b: An open large-scale dataset for training next generation image-text models","Advances inÂ ","2022"
"Exploring plain vision transformer backbones for object detection","European Conference on Computer Vision","2022"
"Visual prompt tuning","Â on Computer Vision","2022"
"Card: Classification and regression diffusion models","Advances in Neural InformationÂ ","2022"
"Swin transformer v2: Scaling up capacity and resolution","Proceedings of theÂ ","2022"
"Coca: Contrastive captioners are image-text foundation models","arXiv preprint arXivÂ ","2022"
"Restormer: Efficient transformer for high-resolution image restoration","Proceedings of theÂ ","2022"
"Lamda: Language models for dialog applications","arXiv preprint arXivÂ ","2022"
"Scaling up your kernels to 31x31: Revisiting large kernel design in cnns","Proceedings of the IEEEÂ ","2022"
"Conditional prompt learning for vision-language models","Proceedings of the IEEEÂ ","2022"
"Masked autoencoders are scalable vision learners","Proceedings of theÂ ","2022"
"Improving language models by retrieving from trillions of tokens","InternationalÂ ","2022"
"Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training","Advances in neuralÂ ","2022"
"[HTML][HTML] Â¡Â°So what if ChatGPT wrote it?Â¡Â± Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for researchÂ Â¡Â¦","International Journal ofÂ ","2023"
"Bevformer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers","European conference onÂ ","2022"
"Scalable diffusion models with transformers","Proceedings of the IEEE/CVFÂ ","2023"
"Masked feature prediction for self-supervised visual pre-training","Proceedings of theÂ ","2022"
"Diffusiondet: Diffusion model for object detection","Proceedings of the IEEEÂ ","2023"
"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models","arXiv preprint arXivÂ ","2022"
"Competition-level code generation with alphacode","Science","2022"
"[HTML][HTML] Discovering faster matrix multiplication algorithms with reinforcement learning","Nature","2022"
"A generalist agent","arXiv preprint arXivÂ ","2022"
"Emergent abilities of large language models","arXiv preprint arXivÂ ","2022"
"Evolutionary-scale prediction of atomic-level protein structure with a language model","Science","2023"
"Masked autoencoders as spatiotemporal learners","Advances in neuralÂ ","2022"
"Segment anything","arXiv preprint arXivÂ ","2023"
"Survey of hallucination in natural language generation","ACM ComputingÂ ","2023"
"Bytetrack: Multi-object tracking by associating every detection box","Â on Computer Vision","2022"
"Grounded language-image pre-training","Proceedings of theÂ ","2022"
"Sparks of artificial general intelligence: Early experiments with gpt-4","arXiv preprint arXivÂ ","2023"
"Mvitv2: Improved multiscale vision transformers for classification and detection","Proceedings of theÂ ","2022"
"Simple baselines for image restoration","European Conference on ComputerÂ ","2022"
"Diffusion-lm improves controllable text generation","Advances in NeuralÂ ","2022"
"Do as i can, not as i say: Grounding language in robotic affordances","arXiv preprint arXivÂ ","2022"
"Wavlm: Large-scale self-supervised pre-training for full stack speech processing","IEEE Journal ofÂ ","2022"
"Metaformer is actually what you need for vision","Proceedings of theÂ ","2022"
"Smoothquant: Accurate and efficient post-training quantization for large language models","InternationalÂ ","2023"
"Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation","Proceedings of theÂ ","2023"
"Flava: A foundational language and vision alignment model","Proceedings of theÂ ","2022"
"Are transformers effective for time series forecasting?","Â of the AAAI conference on artificialÂ ","2023"
"Learning to prompt for vision-language models","International Journal of Computer Vision","2022"
"Swinir: Image restoration using swin transformer","Proceedings of theÂ ","2021"
"Vector quantized diffusion model for text-to-image synthesis","Proceedings of theÂ ","2022"
"TPH-YOLOv5: Improved YOLOv5 based on transformer prediction head for object detection on drone-captured scenarios","Proceedings of the IEEEÂ ","2021"
"Clip-adapter: Better vision-language models with feature adapters","International Journal ofÂ ","2023"
"Inception transformer","Advances in NeuralÂ ","2022"
"Do vision transformers see like convolutional neural networks?","Advances inÂ ","2021"
"On the opportunities and risks of foundation models","arXiv preprint arXivÂ ","2021"
"Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting","Â on Machine Learning","2022"
"Petr: Position embedding transformation for multi-view 3d object detection","European Conference on ComputerÂ ","2022"
"[HTML][HTML] Visual attention network","Computational Visual Media","2023"
"Make-a-scene: Scene-based text-to-image generation with human priors","Â on Computer Vision","2022"
"Transfusion: Robust lidar-camera fusion for 3d object detection with transformers","Proceedings of theÂ ","2022"
"Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing","ACM ComputingÂ ","2023"
"Florence: A new foundation model for computer vision","arXiv preprint arXivÂ ","2021"
"Palette: Image-to-image diffusion models","ACM SIGGRAPH 2022Â ","2022"
"Lit: Zero-shot transfer with locked-image text tuning","Proceedings of theÂ ","2022"
"Robust deep learningâ€“based protein sequence design using ProteinMPNN","Science","2022"
"Dino: Detr with improved denoising anchor boxes for end-to-end object detection","arXiv preprint arXivÂ ","2022"
"[HTML][HTML] Highly accurate protein structure prediction with AlphaFold","Nature","2021"
"Point-bert: Pre-training 3d point cloud transformers with masked point modeling","Proceedings of theÂ ","2022"
"Eva: Exploring the limits of masked visual representation learning at scale","Proceedings of theÂ ","2023"
"Per-pixel classification is not all you need for semantic segmentation","Advances in NeuralÂ ","2021"
"Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model","arXiv preprint arXivÂ ","2022"
"Dn-detr: Accelerate detr training by introducing query denoising","Proceedings of theÂ ","2022"
"Blended diffusion for text-driven editing of natural images","Proceedings of the IEEEÂ ","2022"
"An end-to-end transformer model for 3d object detection","Proceedings of the IEEE/CVFÂ ","2021"
"Ego4d: Around the world in 3,000 hours of egocentric video","Proceedings of theÂ ","2022"
"Early convolutions help transformers see better","Advances in neuralÂ ","2021"
"Denseclip: Language-guided dense prediction with context-aware prompting","Proceedings of theÂ ","2022"
"Artificial intelligence in histopathology: enhancing cancer research and clinical oncology","Nature cancer","2022"
"Beit: Bert pre-training of image transformers","arXiv preprint arXiv:2106.08254","2021"
"Accurate prediction of protein structures and interactions using a three-track neural network","Science","2021"
"[HTML][HTML] Pvt v2: Improved baselines with pyramid vision transformer","Computational VisualÂ ","2022"
"Video swin transformer","Proceedings of theÂ ","2022"
"Internimage: Exploring large-scale vision foundation models with deformable convolutions","Proceedings of theÂ ","2023"
"Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting","Advances in NeuralÂ ","2021"
"Coatnet: Marrying convolution and attention for all data sizes","Advances in neural informationÂ ","2021"
"[HTML][HTML] Large language models encode clinical knowledge","Nature","2023"
"Groupvit: Semantic segmentation emerges from text supervision","Proceedings of theÂ ","2022"
"Lora: Low-rank adaptation of large language models","arXiv preprint arXivÂ ","2021"
"Cswin transformer: A general vision transformer backbone with cross-shaped windows","Proceedings of theÂ ","2022"
"Maxvit: Multi-axis vision transformer","European conference onÂ ","2022"
"Do transformers really perform badly for graph representation?","Advances inÂ ","2021"
"Uformer: A general u-shaped transformer for image restoration","Proceedings of theÂ ","2022"
"Scaling vision transformers","Proceedings of theÂ ","2022"
"Zero-shot text-guided object generation with dream fields","Proceedings of theÂ ","2022"
"SegFormer: Simple and efficient design for semantic segmentation with transformers","Advances inÂ ","2021"
"Decision transformer: Reinforcement learning via sequence modeling","Advances in neuralÂ ","2021"
"[HTML][HTML] A survey of transformers","AI Open","2022"
"Scientific discovery in the age of artificial intelligence","Nature","2023"
"An empirical study of training end-to-end vision-and-language transformers","Proceedings of theÂ ","2022"
"Obtaining genetics insights from deep learning via explainable artificial intelligence","Nature ReviewsÂ ","2023"
"Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning","Advances inÂ ","2022"
"Semi-supervised semantic segmentation with cross pseudo supervision","Proceedings of the IEEEÂ ","2021"
"Quantum advantage in learning from experiments","Science","2022"
"StyleGAN-NADA: CLIP-guided domain adaptation of image generators","ACM Transactions onÂ ","2022"
"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents","Â on Machine Learning","2022"
"Maxim: Multi-axis mlp for image processing","Proceedings of theÂ ","2022"
"Masked siamese networks for label-efficient learning","Â on Computer Vision","2022"
"Swin-unet: Unet-like pure transformer for medical image segmentation","European conference onÂ ","2022"
"Prompt-to-prompt image editing with cross attention control","arXiv preprint arXivÂ ","2022"
"Diffusion models beat gans on image synthesis","Advances in neural informationÂ ","2021"
"Segmenter: Transformer for semantic segmentation","Proceedings of theÂ ","2021"
"Masked autoencoders for point cloud self-supervised learning","European conference onÂ ","2022"
"Multi-concept customization of text-to-image diffusion","Proceedings of theÂ ","2023"
"A comprehensive survey on graph anomaly detection with deep learning","Â on Knowledge andÂ ","2021"
"Conditional detr for fast training convergence","Proceedings of theÂ ","2021"
"Offline reinforcement learning as one big sequence modeling problem","Advances in neural informationÂ ","2021"
"Mlp-mixer: An all-mlp architecture for vision","Advances in neuralÂ ","2021"
"Vlmo: Unified vision-language pre-training with mixture-of-modality-experts","Advances inÂ ","2022"
"Self-supervised learning in medicine and healthcare","Nature Biomedical Engineering","2022"
"Video pretraining (vpt): Learning to act by watching unlabeled online videos","Advances inÂ ","2022"
"Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech","International Conference on MachineÂ ","2021"
"Emerging properties in self-supervised vision transformers","Proceedings of theÂ ","2021"
"Recent advances in natural language processing via large pre-trained language models: A survey","ACM ComputingÂ ","2023"
"SwinFusion: Cross-domain long-range learning for general image fusion via swin transformer","IEEE/CAA Journal ofÂ ","2022"
"Intriguing properties of vision transformers","Advances inÂ ","2021"
"Flashattention: Fast and memory-efficient exact attention with io-awareness","Advances in NeuralÂ ","2022"
"Simvlm: Simple visual language model pretraining with weak supervision","arXiv preprint arXivÂ ","2021"
"Vision transformer with deformable attention","Proceedings of the IEEEÂ ","2022"
"Multimodal few-shot learning with frozen language models","Advances inÂ ","2021"
"How attentive are graph attention networks?","arXiv preprint arXiv:2105.14491","2021"
"Twins: Revisiting the design of spatial attention in vision transformers","Advances inÂ ","2021"
"Simcse: Simple contrastive learning of sentence embeddings","arXiv preprint arXiv:2104.08821","2021"
"Diffusion models in vision: A survey","IEEE Transactions onÂ ","2023"
"The power of scale for parameter-efficient prompt tuning","arXiv preprint arXiv:2104.08691","2021"
"Pay attention to mlps","Advances in Neural InformationÂ ","2021"
"Voxel transformer for 3d object detection","Proceedings of theÂ ","2021"
"Cogview: Mastering text-to-image generation via transformers","Advances inÂ ","2021"
"Transformers in medical imaging: A survey","Medical ImageÂ ","2023"
"Cmt: Convolutional neural networks meet vision transformers","Proceedings of theÂ ","2022"
"Scaling vision transformers to gigapixel images via hierarchical self-supervised learning","Proceedings of theÂ ","2022"
"Mdetr-modulated detection for end-to-end multi-modal understanding","Proceedings of theÂ ","2021"
"Winoground: Probing vision and language models for visio-linguistic compositionality","Proceedings of theÂ ","2022"
"Multiscale vision transformers","Proceedings of theÂ ","2021"
"Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation","arXiv preprint arXiv:2109.00859","2021"
"Perceiver-actor: A multi-task transformer for robotic manipulation","Conference on RobotÂ ","2023"
"Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text","Advances inÂ ","2021"
"Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation","Proceedings of the IEEE/CVFÂ ","2022"
"XLS-R: Self-supervised cross-lingual speech representation learning at scale","arXiv preprint arXivÂ ","2021"
"Image super-resolution via iterative refinement","Â on Pattern AnalysisÂ ","2022"
"Cvt: Introducing convolutions to vision transformers","Proceedings of theÂ ","2021"
"Vivit: A video vision transformer","Proceedings of theÂ ","2021"
"Scaling language-image pre-training via masking","Proceedings of theÂ ","2023"
"A review on the attention mechanism of deep learning","Neurocomputing","2021"
"Styleclip: Text-driven manipulation of stylegan imagery","Proceedings of theÂ ","2021"
"Going deeper with image transformers","Proceedings of theÂ ","2021"
"SpeechBrain: A general-purpose speech toolkit","arXiv preprint arXivÂ ","2021"
"Maskgit: Masked generative image transformer","Proceedings of theÂ ","2022"
"Crossvit: Cross-attention multi-scale vision transformer for image classification","Proceedings of the IEEE/CVFÂ ","2021"
"Swin transformer: Hierarchical vision transformer using shifted windows","Proceedings of theÂ ","2021"
"ChatGPT and other large language models are double-edged swords","Radiology","2023"
"Clipcap: Clip prefix for image captioning","arXiv preprint arXiv:2111.09734","2021"
"Transformer tracking","Proceedings of theÂ ","2021"
"Vision transformers for dense prediction","Proceedings of the IEEEÂ ","2021"
"Learning inverse folding from millions of predicted structures","InternationalÂ ","2022"
"Dynamicvit: Efficient vision transformers with dynamic token sparsification","Advances in neuralÂ ","2021"
"[HTML][HTML] Skilful precipitation nowcasting using deep generative models of radar","Nature","2021"
"LoFTR: Detector-free local feature matching with transformers","Proceedings of theÂ ","2021"
"Are transformers more robust than cnns?","Advances in neuralÂ ","2021"
"Mixformer: End-to-end tracking with iterative mixed attention","Proceedings of the IEEEÂ ","2022"
"Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators","Nature machineÂ ","2021"
"Unetr: Transformers for 3d medical image segmentation","Proceedings of theÂ ","2022"
"A survey of quantization methods for efficient neural network inference","Low-Power ComputerÂ ","2022"
"Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing","arXiv preprint arXiv:2111.09543","2021"
"Convit: Improving vision transformers with soft convolutional inductive biases","InternationalÂ ","2021"
"Image super-resolution with non-local sparse attention","Â of the IEEE/CVF Conference onÂ ","2021"
"SpectralFormer: Rethinking hyperspectral image classification with transformers","Â on Geoscience andÂ ","2021"
"[HTML][HTML] ProtGPT2 is a deep unsupervised language model for protein design","Nature communications","2022"
"Conformer: Local features coupling global representations for visual recognition","Proceedings of theÂ ","2021"
"Attention bottlenecks for multimodal fusion","Advances inÂ ","2021"
"Stratified transformer for 3d point cloud segmentation","Proceedings of theÂ ","2022"
"Hornet: Efficient high-order spatial interactions with recursive gated convolutions","Advances in NeuralÂ ","2022"
"A survey of modern deep learning based object detection models","Digital SignalÂ ","2022"
"Detr3d: 3d object detection from multi-view images via 3d-to-2d queries","Â on Robot Learning","2022"
"Mobile-former: Bridging mobilenet and transformer","Proceedings of theÂ ","2022"
"Structured denoising diffusion models in discrete state-spaces","Advances inÂ ","2021"
"Transformer in transformer","Advances in NeuralÂ ","2021"
"Learning spatio-temporal transformer for visual tracking","Proceedings of the IEEEÂ ","2021"
"Frozen in time: A joint video and image encoder for end-to-end retrieval","Proceedings of theÂ ","2021"
"Rethinking spatial dimensions of vision transformers","Proceedings of theÂ ","2021"
"Zero-shot text-to-image generation","InternationalÂ ","2021"
"Pyramid vision transformer: A versatile backbone for dense prediction without convolutions","Proceedings of theÂ ","2021"
"Social physics","Physics Reports","2022"
"Dynamic head: Unifying object detection heads with attentions","Proceedings of theÂ ","2021"
"Pointr: Diverse point cloud completion with geometry-aware transformers","Proceedings of theÂ ","2021"
"Perceiver: General perception with iterative attention","InternationalÂ ","2021"
"You only look one-level feature","Proceedings of theÂ ","2021"
"Revisiting deep learning models for tabular data","Advances in NeuralÂ ","2021"
"Cliport: What and where pathways for robotic manipulation","Conference on RobotÂ ","2022"
" Is space-time attention all you need for video understanding?","ICML","2021"
"Transunet: Transformers make strong encoders for medical image segmentation","arXiv preprint arXivÂ ","2021"
"Training spiking neural networks using lessons from deep learning","Proceedings of theÂ ","2023"
"Deep neural networks and tabular data: A survey","Â on Neural NetworksÂ ","2022"
"[HTML][HTML] How does ChatGPT perform on the United States medical licensing examination? The implications of large language models for medical education andÂ Â¡Â¦","JMIR MedicalÂ ","2023"
"Transformer meets tracker: Exploiting temporal context for robust visual tracking","Proceedings of the IEEEÂ ","2021"
"[HTML][HTML] Effective gene expression prediction from sequence by integrating long-range interactions","NatureÂ ","2021"
"Large language models generate functional protein sequences across diverse families","NatureÂ ","2023"
"Adaptformer: Adapting vision transformers for scalable visual recognition","Advances inÂ ","2022"
"Tokens-to-token vit: Training vision transformers from scratch on imagenet","Proceedings of theÂ ","2021"
"Observation-centric sort: Rethinking sort for robust multi-object tracking","Proceedings of theÂ ","2023"
"Bottleneck transformers for visual recognition","Proceedings of theÂ ","2021"
"Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts","Proceedings of theÂ ","2021"
"On the dangers of stochastic parrots: Can language models be too big?ðŸ¦œ","Proceedings of the 2021Â ","2021"
"Transreid: Transformer-based object re-identification","Proceedings of theÂ ","2021"
"Directed evolution: methodologies and applications","Chemical reviews","2021"
"Cogview2: Faster and better text-to-image generation via hierarchical transformers","Advances in NeuralÂ ","2022"
"Ibrnet: Learning multi-view image-based rendering","Proceedings of theÂ ","2021"
"You only learn one representation: Unified network for multiple tasks","arXiv preprint arXiv:2105.04206","2021"
"Pointclip: Point cloud understanding by clip","Proceedings of theÂ ","2022"
"Cross-view transformers for real-time map-view semantic segmentation","Â of the IEEE/CVF conference onÂ ","2022"
"Extract free dense labels from clip","European Conference on Computer Vision","2022"
"Multi-game decision transformers","Advances inÂ ","2022"
"Vitae: Vision transformer advanced by exploring intrinsic inductive bias","Advances in neuralÂ ","2021"
"Recipe for a general, powerful, scalable graph transformer","Advances inÂ ","2022"
"Ast: Audio spectrogram transformer","arXiv preprint arXiv:2104.01778","2021"
"Focal self-attention for local-global interactions in vision transformers","arXiv preprint arXivÂ ","2021"
"Incorporating convolution designs into visual transformers","Proceedings of theÂ ","2021"
"Learning to prompt for continual learning","Proceedings of theÂ ","2022"
"Diffusionclip: Text-guided diffusion models for robust image manipulation","Â of the IEEE/CVF Conference onÂ ","2022"
"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity","The Journal of Machine LearningÂ ","2022"
"Barf: Bundle-adjusting neural radiance fields","Proceedings of the IEEEÂ ","2021"
"Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning","Neurocomputing","2022"
"Learning transferable visual models from natural language supervision","InternationalÂ ","2021"
"Less is more: Clipbert for video-and-language learning via sparse sampling","Proceedings of theÂ ","2021"
"Transformers in vision: A survey","ACM computingÂ ","2022"
"Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers","Proceedings of theÂ ","2021"
"Prefix-tuning: Optimizing continuous prompts for generation","arXiv preprint arXiv:2101.00190","2021"
"Levit: a vision transformer in convnet's clothing for faster inference","Proceedings of theÂ ","2021"
"Perceiver io: A general architecture for structured inputs & outputs","arXiv preprint arXivÂ ","2021"
"Large language models struggle to learn long-tail knowledge","InternationalÂ ","2023"
"Localvit: Bringing locality to vision transformers","arXiv preprint arXivÂ ","2021"
"Transmil: Transformer based correlated multiple instance learning for whole slide image classification","Advances in neuralÂ ","2021"
"High-performance large-scale image recognition without normalization","Â Conference on MachineÂ ","2021"
"Training data-efficient image transformers & distillation through attention","InternationalÂ ","2021"
"Merlot: Multimodal neural script knowledge models","Advances inÂ ","2021"
"Open-vocabulary object detection via vision and language knowledge distillation","arXiv preprint arXiv:2104.13921","2021"
"Transfuse: Fusing transformers and cnns for medical image segmentation","Medical Image Computing and Computer AssistedÂ ","2021"
"Taming transformers for high-resolution image synthesis","Proceedings of the IEEEÂ ","2021"
"Point transformer","Proceedings of theÂ ","2021"
"Vision-language pre-training with triple contrastive learning","Proceedings of theÂ ","2022"
"Informer: Beyond efficient transformer for long sequence time-series forecasting","Proceedings of theÂ ","2021"
"A survey on vision transformer","IEEE transactions onÂ ","2022"
"Convnext v2: Co-designing and scaling convnets with masked autoencoders","Proceedings of theÂ ","2023"
"Pct: Point cloud transformer","Computational VisualÂ ","2021"
"Trackformer: Multi-object tracking with transformers","Proceedings of theÂ ","2022"
"Glipv2: Unifying localization and vision-language understanding","Advances inÂ ","2022"
"Emergent analogical reasoning in large language models","Nature Human Behaviour","2023"
"Extracting training data from large language models","30th USENIX SecurityÂ ","2021"
"Deep bidirectional language-knowledge graph pretraining","Advances inÂ ","2022"
"Merlot reserve: Neural script knowledge through vision and language and sound","Proceedings of theÂ ","2022"
"Deepvit: Towards deeper vision transformer","arXiv preprint arXivÂ ","2021"
"Vision transformer adapter for dense predictions","arXiv preprint arXivÂ ","2022"
"Understanding the robustness in vision transformers","InternationalÂ ","2022"
"Program synthesis with large language models","arXiv preprint arXivÂ ","2021"
"Pre-trained image processing transformer","Proceedings of theÂ ","2021"
"Scaling local self-attention for parameter efficient visual backbones","Proceedings of theÂ ","2021"
"Actionformer: Localizing moments of actions with transformers","European Conference on Computer Vision","2022"
"BioGPT: generative pre-trained transformer for biomedical text generation and mining","Briefings inÂ ","2022"
"Multiview transformers for video recognition","Proceedings of theÂ ","2022"
"Multi-modal fusion transformer for end-to-end autonomous driving","Proceedings of the IEEEÂ ","2021"
"Sparse r-cnn: End-to-end object detection with learnable proposals","Proceedings of theÂ ","2021"
"Scaling vision with sparse mixture of experts","Advances inÂ ","2021"
"Rethinking semantic segmentation: A prototype view","Proceedings of theÂ ","2022"
"Unifying vision-and-language tasks via text generation","International Conference onÂ ","2021"
"Deit iii: Revenge of the vit","European Conference on Computer Vision","2022"
"[HTML][HTML] Multimodal biomedical AI","Nature Medicine","2022"
"Real-world robot learning with masked visual pre-training","Â on Robot Learning","2023"
"Videoclip: Contrastive pre-training for zero-shot video-text understanding","arXiv preprint arXivÂ ","2021"
"Graph neural network for traffic forecasting: A survey","Expert Systems with Applications","2022"
"Unsupervised speech recognition","Advances in NeuralÂ ","2021"
"Fastnerf: High-fidelity neural rendering at 200fps","Proceedings of theÂ ","2021"
"End-to-end video instance segmentation with transformers","Proceedings of theÂ ","2021"
"Vision gnn: An image is worth graph of nodes","Advances in NeuralÂ ","2022"
"D-nerf: Neural radiance fields for dynamic scenes","Proceedings of theÂ ","2021"
"Resmlp: Feedforward networks for image classification with data-efficient training","Â on Pattern AnalysisÂ ","2022"
"[HTML][HTML] A review of uncertainty quantification in deep learning: Techniques, applications and challenges","Information fusion","2021"
"Davit: Dual attention vision transformers","European Conference onÂ ","2022"
"[HTML][HTML] Geometry-enhanced molecular representation learning for property prediction","Nature MachineÂ ","2022"
"Fcanet: Frequency channel attention networks","Proceedings of the IEEE/CVFÂ ","2021"
"Conditional positional encodings for vision transformers","arXiv preprint arXivÂ ","2021"
"Unified pre-training for program understanding and generation","arXiv preprint arXivÂ ","2021"
"Ota: Optimal transport assignment for object detection","Proceedings of the IEEEÂ ","2021"
"Transformers in time series: A survey","arXiv preprint arXivÂ ","2022"
"Remote sensing image change detection with transformers","IEEE Transactions on Geoscience andÂ ","2021"
"Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks","The Journal of MachineÂ ","2021"
"Dynamic detr: End-to-end object detection with dynamic attention","Proceedings of theÂ ","2021"
"Global filter networks for image classification","Advances in neuralÂ ","2021"
"Unified contrastive learning in image-text-label space","Proceedings of theÂ ","2022"
"Underspecification presents challenges for credibility in modern machine learning","The Journal of MachineÂ ","2022"
"mT5: A massively multilingual pre-trained text-to-text transformer","arXiv preprint arXivÂ ","2020"
"Neural operator: Learning maps between function spaces","arXiv preprint arXivÂ ","2021"
"End-to-end human pose and mesh reconstruction with transformers","Â of the IEEE/CVF conference onÂ ","2021"
"Codexglue: A machine learning benchmark dataset for code understanding and generation","arXiv preprint arXivÂ ","2021"
"Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction","Proceedings of theÂ ","2021"
"Dynamic neural networks: A survey","IEEE Transactions onÂ ","2021"
"CryoDRGN: reconstruction of heterogeneous cryo-EM structures using neural networks","Nature methods","2021"
"Video transformer network","Proceedings of theÂ ","2021"
"Locating and editing factual associations in GPT","Advances in NeuralÂ ","2022"
"Spectralâ€“spatial feature tokenization transformer for hyperspectral image classification","IEEE Transactions onÂ ","2022"
"Torsional diffusion for molecular conformer generation","Advances in NeuralÂ ","2022"
"Diverse part discovery: Occluded person re-identification with part-aware transformer","Proceedings of theÂ ","2021"
"Understanding robustness of transformers for image classification","Proceedings of theÂ ","2021"
"Rethinking graph transformers with spectral attention","Advances inÂ ","2021"
"Deformable detr: Deformable transformers for end-to-end object detection","arXiv preprint arXiv:2010.04159","2020"
"Harnessing multimodal data integration to advance precision oncology","Nature ReviewsÂ ","2022"
"V2x-vit: Vehicle-to-everything cooperative perception with vision transformer","European conference onÂ ","2022"
"Revisiting resnets: Improved training and scaling strategies","Advances inÂ ","2021"
"Human-level play in the game of Diplomacy by combining language models with strategic reasoning","Science","2022"
"Using cognitive psychology to understand GPT-3","Proceedings of the National Academy ofÂ ","2023"
"Transbts: Multimodal brain tumor segmentation using transformer","Medical Image ComputingÂ ","2021"
"Embracing single stride 3d object detector with sparse transformer","Proceedings of theÂ ","2022"
"DeepLoc 2.0: multi-label subcellular localization prediction using protein language models","Nucleic AcidsÂ ","2022"
"Adbench: Anomaly detection benchmark","Advances in NeuralÂ ","2022"
"Transformer interpretability beyond attention visualization","Â of the IEEE/CVF conference onÂ ","2021"
"Rethinking attention with performers","arXiv preprint arXivÂ ","2020"
"Combining machine learning and computational chemistry for predictive insights into chemical systems","ChemicalÂ ","2021"
"Mat: Mask-aware transformer for large hole image inpainting","Proceedings of theÂ ","2022"
"NÃ¼wa: Visual synthesis pre-training for neural visual world creation","European conference onÂ ","2022"
"Co-scale conv-attentional image transformers","Proceedings of the IEEE/CVFÂ ","2021"
"[HTML][HTML] Pre-trained models: Past, present and future","AI Open","2021"
"[HTML][HTML] Intelligent metasurfaces: control, communication and computing","Elight","2022"
"U-net and its variants for medical image segmentation: A review of theory and applications","IeeeÂ ","2021"
"Max-deeplab: End-to-end panoptic segmentation with mask transformers","Proceedings of theÂ ","2021"
"Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm","arXiv preprint arXivÂ ","2021"
"On the integration of self-attention and convolution","Proceedings of theÂ ","2022"
"Visual saliency transformer","Proceedings of the IEEEÂ ","2021"
"Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action","Conference on RobotÂ ","2023"
"Contextual transformer networks for visual recognition","IEEE Transactions on PatternÂ ","2022"
"Training compute-optimal large language models","arXiv preprint arXivÂ ","2022"
"Generative adversarial networks in medical image augmentation: A review","Computers in BiologyÂ ","2022"
"Rethinking and improving relative position encoding for vision transformer","Proceedings of the IEEEÂ ","2021"
"Conversational agents in therapeutic interventions for neurodevelopmental disorders: a survey","ACM Computing Surveys","2023"
"Transforming model prediction for tracking","Proceedings of theÂ ","2022"
"Gan inversion: A survey","IEEE Transactions onÂ ","2022"
"Motr: End-to-end multiple-object tracking with transformer","European Conference onÂ ","2022"
"Geometric transformer for fast and robust point cloud registration","Proceedings of theÂ ","2022"
"Up-detr: Unsupervised pre-training for object detection with transformers","Â of the IEEE/CVF conference onÂ ","2021"
"Rethinking network design and local geometry in point cloud: A simple residual MLP framework","arXiv preprint arXiv:2202.07123","2022"
"Blended latent diffusion","ACM Transactions on GraphicsÂ ","2023"
"Bevt: Bert pretraining of video transformers","Proceedings of theÂ ","2022"
"Large language models are human-level prompt engineers","arXiv preprint arXivÂ ","2022"
"Beyond english-centric multilingual machine translation","The Journal of MachineÂ ","2021"
"Vision-language pre-training: Basics, recent advances, and future trends","Foundations and TrendsÂ ","2022"
"Transgan: Two pure transformers can make one strong gan, and that can scale up","Advances in NeuralÂ ","2021"
"What Makes Good In-Context Examples for GPT-?","arXiv preprint arXivÂ ","2021"
"Dualprompt: Complementary prompting for rehearsal-free continual learning","Â on Computer Vision","2022"
"Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting","Proceedings of the IEEEÂ ","2021"
"Fnet: Mixing tokens with fourier transforms","arXiv preprint arXivÂ ","2021"
"It's not just size that matters: Small language models are also few-shot learners","arXiv preprint arXiv:2009.07118","2020"
"Swintrack: A simple and strong baseline for transformer tracking","Advances in NeuralÂ ","2022"
"The impact of AlphaFold2 one year on","Nature methods","2022"
"Escaping the big data paradigm with compact transformers","arXiv preprint arXivÂ ","2021"
"Advances in neural rendering","Computer GraphicsÂ ","2022"
"Single-sequence protein structure prediction using a language model and deep learning","NatureÂ ","2022"
"Patches are all you need?","arXiv preprint arXiv:2201.09792","2022"
"A survey of human-in-the-loop for machine learning","Future Generation ComputerÂ ","2022"
"Transformer for single image super-resolution","Proceedings of theÂ ","2022"
"Big bird: Transformers for longer sequences","Advances in neuralÂ ","2020"
"Domain-specific language model pretraining for biomedical natural language processing","ACM Transactions onÂ ","2021"
"Efficientformer: Vision transformers at mobilenet speed","Advances inÂ ","2022"
"Multimae: Multi-modal multi-task masked autoencoders","European Conference onÂ ","2022"
"How to build a cognitive map","NatureÂ ","2022"
"Involution: Inverting the inherence of convolution for visual recognition","Proceedings of theÂ ","2021"
"Volo: Vision outlooker for visual recognition","IEEE transactions onÂ ","2022"
"Adabelief optimizer: Adapting stepsizes by the belief in observed gradients","Advances in neuralÂ ","2020"
"Luke: deep contextualized entity representations with entity-aware self-attention","arXiv preprint arXivÂ ","2020"
"Diffwave: A versatile diffusion model for audio synthesis","arXiv preprint arXivÂ ","2020"
"Transtrack: Multiple object tracking with transformer","arXiv preprint arXivÂ ","2020"
"You only look at one sequence: Rethinking transformer in vision through object detection","Advances inÂ ","2021"
"[HTML][HTML] Ranking the effectiveness of worldwide COVID-19 government interventions","Nature humanÂ ","2020"
"Neural sparse voxel fields","Advances in NeuralÂ ","2020"
"Layoutlmv3: Pre-training for document ai with unified text and image masking","Proceedings of the 30th ACMÂ ","2022"
"Physdiff: Physics-guided human motion diffusion model","Proceedings of theÂ ","2023"
"Flowformer: A transformer architecture for optical flow","Â on Computer Vision","2022"
"Holistic evaluation of language models","arXiv preprint arXivÂ ","2022"
"Mpvit: Multi-path vision transformer for dense prediction","Proceedings of the IEEEÂ ","2022"
"Improving 3d object detection with channel-wise transformer","Proceedings of theÂ ","2021"
" Recent advances in end-to-end automatic speech recognition","APSIPA Transactions on Signal and InformationÂ ","2022"
"Prottrans: Toward understanding the language of life through self-supervised learning","IEEE transactions onÂ ","2021"
"wav2vec 2.0: A framework for self-supervised learning of speech representations","Advances in neuralÂ ","2020"
"Dytox: Transformers for continual learning with dynamic token expansion","Proceedings of theÂ ","2022"
"Denoising diffusion probabilistic models","Advances in neural informationÂ ","2020"
"Transformers are rnns: Fast autoregressive transformers with linear attention","Â on machine learning","2020"
"Byt5: Towards a token-free future with pre-trained byte-to-byte models","Transactions of theÂ ","2022"
"Expanding language-image pretrained models for general video recognition","Â on Computer Vision","2022"
"On the sentence embeddings from pre-trained language models","arXiv preprint arXiv:2011.05864","2020"
"Dab-detr: Dynamic anchor boxes are better queries for detr","arXiv preprint arXivÂ ","2022"
"A generalization of transformer networks to graphs","arXiv preprint arXiv:2012.09699","2020"
"Fourier features let networks learn high frequency functions in low dimensional domains","Advances inÂ ","2020"
"Generative pretraining from pixels","InternationalÂ ","2020"
"Visual prompting via image inpainting","Advances in NeuralÂ ","2022"
"[HTML][HTML] Earthquake transformerâ€”an attentive deep-learning model for simultaneous earthquake detection and phase picking","NatureÂ ","2020"
"Efficient large-scale language model training on gpu clusters using megatron-lm","Proceedings of theÂ ","2021"
"Omnivore: A single model for many visual modalities","Proceedings of theÂ ","2022"
"Action-conditioned 3D human motion synthesis with transformer VAE","Proceedings of the IEEEÂ ","2021"
"Unit: Multimodal multitask learning with a unified transformer","Proceedings of the IEEE/CVF InternationalÂ ","2021"
"Self-supervised learning: Generative or contrastive","IEEE transactions onÂ ","2021"
"LEGAL-BERT: The muppets straight out of law school","arXiv preprint arXivÂ ","2020"
"TEMOS: Generating diverse human motions from textual descriptions","European Conference on ComputerÂ ","2022"
"Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images","International MICCAIÂ ","2021"
"Pali: A jointly-scaled multilingual language-image model","arXiv preprint arXivÂ ","2022"
"Roformer: Enhanced transformer with rotary position embedding","arXiv preprint arXivÂ ","2021"
"W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training","2021 IEEE AutomaticÂ ","2021"
"Ds-transunet: Dual swin transformer u-net for medical image segmentation","IEEE Transactions onÂ ","2022"
"Deberta: Decoding-enhanced bert with disentangled attention","arXiv preprint arXiv:2006.03654","2020"
"Transfuser: Imitation with transformer-based sensor fusion for autonomous driving","Â on Pattern AnalysisÂ ","2022"
"Pure transformers are powerful graph learners","Advances in NeuralÂ ","2022"
"End-to-end object detection with transformers","European conference onÂ ","2020"
"Walk in the cloud: Learning curves for point clouds shape analysis","Proceedings of theÂ ","2021"
"Linformer: Self-attention with linear complexity","arXiv preprint arXiv:2006.04768","2020"
"Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things","IEEE Internet of Things Journal","2020"
"Graph neural networks for natural language processing: A survey","Â and TrendsÂ® inÂ ","2023"
"Fastspeech 2: Fast and high-quality end-to-end text to speech","arXiv preprint arXivÂ ","2020"
"Versatile diffusion: Text, images and variations all in one diffusion model","Proceedings of theÂ ","2023"
"Seeing out of the box: End-to-end pre-training for vision-language representation learning","Proceedings of theÂ ","2021"
"Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning","Â of the IEEE/CVF conference onÂ ","2021"
"Explainability in graph neural networks: A taxonomic survey","IEEE transactions on patternÂ ","2022"
"On-device training under 256kb memory","Advances in NeuralÂ ","2022"
"Keeping your eye on the ball: Trajectory attention in video transformers","Advances in neuralÂ ","2021"
"Conformer: Convolution-augmented transformer for speech recognition","arXiv preprint arXivÂ ","2020"
"Gcc: Graph contrastive coding for graph neural network pre-training","Proceedings of the 26thÂ ","2020"
"Large language models encode clinical knowledge","arXiv preprint arXivÂ ","2022"
"How much can clip benefit vision-and-language tasks?","arXiv preprint arXivÂ ","2021"
"Learning to summarize with human feedback","Advances inÂ ","2020"
"Zero-shot image-to-image translation","ACM SIGGRAPH 2023Â ","2023"
"Towards language-free training for text-to-image generation","Proceedings of theÂ ","2022"
"Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond","International Journal of Computer Vision","2023"
"Learning texture transformer network for image super-resolution","Proceedings of the IEEEÂ ","2020"
"[HTML][HTML] Deep Learning applications for COVID-19","Journal of big Data","2021"
"Grand: Graph neural diffusion","InternationalÂ ","2021"
"History aware multimodal transformer for vision-and-language navigation","Advances in neuralÂ ","2021"
" Learning the protein language: Evolution, structure, and function","Cell systems","2021"
"Realtoxicityprompts: Evaluating neural toxic degeneration in language models","arXiv preprint arXivÂ ","2020"
"Distilling knowledge via knowledge review","Proceedings of the IEEE/CVFÂ ","2021"
"Graphcodebert: Pre-training code representations with data flow","arXiv preprint arXivÂ ","2020"
"A survey on neural speech synthesis","arXiv preprint arXiv:2106.15561","2021"
"Efficiently teaching an effective dense retriever with balanced topic aware sampling","Proceedings of the 44thÂ ","2021"
"Anchor detr: Query design for transformer-based detector","Â of the AAAI conference on artificialÂ ","2022"
"Mask dino: Towards a unified transformer-based framework for object detection and segmentation","Proceedings of theÂ ","2023"
"Mhformer: Multi-hypothesis transformer for 3d human pose estimation","Proceedings of theÂ ","2022"
"[HTML][HTML] Multimodal neurons in artificial neural networks","Distill","2021"
"Dynabench: Rethinking benchmarking in NLP","arXiv preprint arXivÂ ","2021"
"Machine Learning for industrial applications: A comprehensive literature review","Expert Systems withÂ ","2021"
"Don't stop pretraining: Adapt language models to domains and tasks","arXiv preprint arXivÂ ","2020"
"Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation","Â , France, September 27â€“October 1, 2021Â ","2021"
"Gmflow: Learning optical flow via global matching","Proceedings of theÂ ","2022"
"Transfg: A transformer architecture for fine-grained recognition","Proceedings of theÂ ","2022"
"Object-centric learning with slot attention","Advances inÂ ","2020"
"Learning to estimate hidden motions with global motion aggregation","Proceedings of theÂ ","2021"
"Attention is not all you need: Pure attention loses rank doubly exponentially with depth","Â Conference on MachineÂ ","2021"
"[HTML][HTML] Drug discovery with explainable artificial intelligence","Nature Machine Intelligence","2020"
"Camouflaged object segmentation with distraction mining","Proceedings of theÂ ","2021"
"Trocr: Transformer-based optical character recognition with pre-trained models","Proceedings of theÂ ","2023"
"Attentional feature fusion","Proceedings of theÂ ","2021"
"Longformer: The long-document transformer","arXiv preprint arXiv:2004.05150","2020"
"Transformer quality in linear time","International Conference onÂ ","2022"
"Crypten: Secure multi-party computation meets machine learning","Advances inÂ ","2021"
"Coderl: Mastering code generation through pretrained models and deep reinforcement learning","Advances in NeuralÂ ","2022"
"Long-tailed classification by keeping the good and removing the bad momentum causal effect","Advances in NeuralÂ ","2020"
"A transformer-based framework for multivariate time series representation learning","Proceedings of the 27thÂ ","2021"
"3d object detection with pointformer","Proceedings of the IEEEÂ ","2021"
"Diffusion SchrÃ¶dinger bridge with applications to score-based generative modeling","Advances in NeuralÂ ","2021"
"Brain tumor segmentation based on the fusion of deep semantics and edge information in multimodal MRI","Information Fusion","2023"
"Point transformer v2: Grouped vector attention and partition-based pooling","Advances in NeuralÂ ","2022"
"Contrastive representation learning: A framework and review","Ieee Access","2020"
"Self-supervised hypergraph convolutional networks for session-based recommendation","Proceedings of the AAAIÂ ","2021"
"Spanish pre-trained bert model and evaluation data","arXiv preprint arXivÂ ","2023"
"Gligen: Open-set grounded text-to-image generation","Proceedings of theÂ ","2023"
"Colbert: Efficient and effective passage search via contextualized late interaction over bert","Proceedings of the 43rd International ACM SIGIRÂ ","2020"
"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks","Computer Visionâ€“ECCVÂ ","2020"
"ProteinBERT: a universal deep-learning model of protein sequence and function","N Brandes, D Ofer, Y Peleg, N RappoportÂ - , 2022 - academic.oup.com","2022"
"Multi-modal transformer for video retrieval","Â 28, 2020, Proceedings, Part IV 16","2020"
"Shortcut learning in deep neural networks","Nature MachineÂ ","2020"
"Deep learning--based text classification: a comprehensive review","ACM computingÂ ","2021"
"Multiple knowledge representation for big data artificial intelligence: framework, applications, and case studies","Frontiers of Information Technology &Â ","2021"
"A spatial-temporal attention-based method and a new dataset for remote sensing image change detection","Remote Sensing","2020"
"An attention-based deep learning approach for sleep stage classification with single-channel EEG","Â on Neural SystemsÂ ","2021"
"[HTML][HTML] Review of image classification algorithms based on convolutional neural networks","Remote Sensing","2021"
"[HTML][HTML] ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports","EuropeanÂ ","2023"
"Putting nerf on a diet: Semantically consistent few-shot view synthesis","Proceedings of the IEEE/CVFÂ ","2021"
"Geometric deep learning on molecular representations","Nature Machine Intelligence","2021"
"Scale-mae: A scale-aware masked autoencoder for multiscale geospatial representation learning","Proceedings of theÂ ","2023"
"Autoformer: Searching transformers for visual recognition","Proceedings of the IEEEÂ ","2021"
"Wavegrad: Estimating gradients for waveform generation","arXiv preprint arXivÂ ","2020"
"Unsupervised cross-lingual representation learning for speech recognition","arXiv preprint arXivÂ ","2020"
"Pre-trained models for natural language processing: A survey","Science ChinaÂ ","2020"
"Recipes for building an open-domain chatbot","arXiv preprint arXivÂ ","2020"
"Ilvr: Conditioning method for denoising diffusion probabilistic models","arXiv preprint arXiv:2108.02938","2021"
"Exploring self-attention for image recognition","Â of the IEEE/CVF conference onÂ ","2020"
"BLEURT: Learning robust metrics for text generation","arXiv preprint arXiv:2004.04696","2020"
"Docformer: End-to-end transformer for document understanding","Proceedings of theÂ ","2021"
"Self-supervised graph transformer on large-scale molecular data","Advances inÂ ","2020"
"UTNet: a hybrid transformer architecture for medical image segmentation","Â , France, September 27â€“October 1, 2021Â ","2021"
"Nerf: Representing scenes as neural radiance fields for view synthesis","Communications of theÂ ","2021"
"[HTML][HTML] Foundation models for generalist medical artificial intelligence","Nature","2023"
"Se (3)-transformers: 3d roto-translation equivariant attention networks","Advances in neuralÂ ","2020"
"Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models","IEEE transactions onÂ ","2021"
"Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation","InternationalÂ ","2020"
"[HTML][HTML] Machine learning in protein structure prediction","Current opinion in chemical biology","2021"
"Musiq: Multi-scale image quality transformer","Proceedings of theÂ ","2021"
"Language-agnostic BERT sentence embedding","arXiv preprint arXivÂ ","2020"
"Cris: Clip-driven referring image segmentation","Proceedings of theÂ ","2022"
"Rethinking transformer-based set prediction for object detection","Proceedings of the IEEEÂ ","2021"
"Pyramid r-cnn: Towards better performance and adaptability for 3d object detection","Proceedings of theÂ ","2021"
"A primer in BERTology: What we know about how BERT works","Transactions of the AssociationÂ ","2021"
"On faithfulness and factuality in abstractive summarization","arXiv preprint arXivÂ ","2020"
"Morel: Model-based offline reinforcement learning","Advances in neuralÂ ","2020"
"Chatting and cheating: Ensuring academic integrity in the era of ChatGPT","Innovations in EducationÂ ","2023"
"Snowflakenet: Point cloud completion by snowflake point deconvolution with skip-transformer","Proceedings of theÂ ","2021"
"Language models are few-shot learners","Advances in neuralÂ ","2020"
"Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training","Advances in neuralÂ ","2022"
"Codebert: A pre-trained model for programming and natural languages","arXiv preprint arXivÂ ","2020"
"Rethinking vision transformers for mobilenet size and speed","Proceedings of theÂ ","2023"
"Long range arena: A benchmark for efficient transformers","arXiv preprint arXivÂ ","2020"
"Unixcoder: Unified cross-modal pre-training for code representation","arXiv preprint arXivÂ ","2022"
"Muse: Text-to-image generation via masked generative transformers","arXiv preprint arXivÂ ","2023"
"Predator: Registration of 3d point clouds with low overlap","Proceedings of theÂ ","2021"
"NystrÃ¶mformer: A nystrÃ¶m-based algorithm for approximating self-attention","Proceedings of theÂ ","2021"
"Rstnet: Captioning with adaptive attention on visual and non-visual words","Proceedings of theÂ ","2021"
"Vectornet: Encoding hd maps and agent dynamics from vectorized representation","Proceedings of theÂ ","2020"
"[HTML][HTML] Catalyzing next-generation artificial intelligence through neuroai","NatureÂ ","2023"
"Snowflake point deconvolution for point cloud completion and generation with skip-transformer","Â on Pattern AnalysisÂ ","2022"
"Axial-deeplab: Stand-alone axial-attention for panoptic segmentation","European conference onÂ ","2020"
"Graphmae: Self-supervised masked graph autoencoders","Proceedings of the 28thÂ ","2022"
"Instances as queries","Proceedings of theÂ ","2021"
"SemEval-2020 task 12: Multilingual offensive language identification in social media (OffensEval 2020)","arXiv preprint arXivÂ ","2020"
"S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization","Proceedings of the 29thÂ ","2020"
"Retrieval-augmented generation for knowledge-intensive nlp tasks","Advances inÂ ","2020"
"Revisiting pre-trained models for Chinese natural language processing","arXiv preprint arXivÂ ","2020"
"Rest: An efficient transformer for visual recognition","Advances in neural informationÂ ","2021"
"A-vit: Adaptive tokens for efficient vision transformer","Proceedings of theÂ ","2022"
"Global tracking transformers","Proceedings of the IEEEÂ ","2022"
"MSA transformer","InternationalÂ ","2021"
" Benchmarking graph neural networks","arXiv preprint arXivÂ ","2020"
"Multilingual denoising pre-training for neural machine translation","Transactions of theÂ ","2020"
"Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition","Proceedings of theÂ ","2022"
"Mpnet: Masked and permuted pre-training for language understanding","Advances in NeuralÂ ","2020"
"Motionclip: Exposing human motion generation to clip space","Â on Computer Vision","2022"
"Arabert: Transformer-based model for arabic language understanding","arXiv preprint arXiv:2003.00104","2020"
"Ai choreographer: Music conditioned 3d dance generation with aist++","Proceedings of the IEEEÂ ","2021"
"Uniformer: Unifying convolution and self-attention for visual recognition","Â on Pattern AnalysisÂ ","2023"
"Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation","Proceedings of theÂ ","2020"
"Summeval: Re-evaluating summarization evaluation","Transactions of theÂ ","2021"
"Mesh graphormer","Proceedings of the IEEE/CVFÂ ","2021"
"Semantic communications for future internet: Fundamentals, applications, and challenges","Â Surveys & Tutorials","2022"
"Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers","Advances in NeuralÂ ","2020"
"BERTimbau: pretrained BERT models for Brazilian Portuguese","Â 2020, Rio Grande, Brazil, October 20â€“23Â ","2020"
"Fast convergence of detr with spatially modulated co-attention","Proceedings of the IEEEÂ ","2021"
"Patching open-vocabulary models by interpolating weights","Advances inÂ ","2022"
"Machine learning and AI in marketingâ€“Connecting computing power to human insights","International Journal of Research in Marketing","2020"
"X-linear attention networks for image captioning","Â of the IEEE/CVF conference onÂ ","2020"
"Deep learning enabled semantic communication systems","IEEE Transactions on SignalÂ ","2021"
"A survey on knowledge graph-based recommender systems","Â on Knowledge andÂ ","2020"
"End-to-end generative pretraining for multimodal video captioning","Proceedings of theÂ ","2022"
"Emotion recognition from speech using wav2vec 2.0 embeddings","arXiv preprint arXiv:2104.03502","2021"
"Exploring and distilling posterior and prior knowledge for radiology report generation","Proceedings of the IEEEÂ ","2021"
"Sequential recommendation with graph neural networks","Proceedings of the 44thÂ ","2021"
"Large-scale adversarial training for vision-and-language representation learning","Advances in NeuralÂ ","2020"
"NTIRE 2023 image shadow removal challenge report","Proceedings of theÂ ","2023"
"Generalizing face forgery detection with high-frequency features","Proceedings of the IEEEÂ ","2021"
"Stmtrack: Template-free visual tracking with space-time memory networks","Â of the IEEE/CVF conference onÂ ","2021"
"Laplace redux-effortless bayesian deep learning","Advances inÂ ","2021"
"Clear: Contrastive learning for sentence representation","arXiv preprint arXivÂ ","2020"
"Aiatrack: Attention in attention for transformer visual tracking","European Conference onÂ ","2022"
"Multimodal motion prediction with stacked transformers","Proceedings of theÂ ","2021"
"Cotr: Correspondence transformer for matching across images","Proceedings of theÂ ","2021"
"[ÃƒÂ¥][B] Pretrained transformers for text ranking: Bert and beyond","J Lin, R Nogueira, A Yates - 2022 - books.google.com","2022"
"Generative adversarial transformers","International conference on machineÂ ","2021"
"Vision transformers for remote sensing image classification","Remote Sensing","2021"
"BERTweet: A pre-trained language model for English Tweets","arXiv preprint arXiv:2005.10200","2020"
"Pegasus: Pre-training with extracted gap-sentences for abstractive summarization","Â Conference on MachineÂ ","2020"
"The evolution, evolvability and engineering of gene regulatory DNA","Nature","2022"
"Heterogeneous graph transformer","Proceedings of the web conference 2020","2020"
"Galactica: A large language model for science","arXiv preprint arXivÂ ","2022"
"Visual transformers: Token-based image representation and processing for computer vision","arXiv preprint arXivÂ ","2020"
"Align and prompt: Video-and-language pre-training with entity prompts","Proceedings of the IEEEÂ ","2022"
"Visual chatgpt: Talking, drawing and editing with visual foundation models","arXiv preprint arXivÂ ","2023"
"Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models","arXiv preprint arXiv:2106.10199","2021"
"E-CRF: Embedded Conditional Random Field for Boundary-caused Class Weights Confusion in Semantic Segmentation","arXiv preprint arXiv:2112.07106","2021"
"Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation","International Conference onÂ ","2023"
"A generalist framework for panoptic segmentation of images and videos","Proceedings of theÂ ","2023"
"Explainable deep learning models in medical image analysis","Journal of imaging","2020"
"[HTML][HTML] Shifting machine learning for healthcare from development to deployment and from models to data","Nature Biomedical Engineering","2022"
"Ssast: Self-supervised audio spectrogram transformer","Â of the AAAI Conference on ArtificialÂ ","2022"
"Normalizing flows for probabilistic modeling and inference","The Journal of MachineÂ ","2021"
"A simple language model for task-oriented dialogue","Advances in NeuralÂ ","2020"
"[ÃƒÂ¥][B] The principles of deep learning theory","DA Roberts, S Yaida, B Hanin - 2022 - cambridge.org","2022"
"Superglue: Learning feature matching with graph neural networks","Proceedings of theÂ ","2020"
"Meshed-memory transformer for image captioning","Proceedings of theÂ ","2020"
"3d-aware image synthesis via learning structural and textural representations","Proceedings of theÂ ","2022"
"Jcs: An explainable covid-19 diagnosis system by joint classification and segmentation","Â on Image Processing","2021"
"Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling","Proceedings of the IEEEÂ ","2020"
"Quantifying attention flow in transformers","arXiv preprint arXiv:2005.00928","2020"
"Deep residual learning in spiking neural networks","Advances in NeuralÂ ","2021"
"Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition","Proceedings of theÂ ","2021"
"Deepfakes and beyond: A survey of face manipulation and fake detection","InformationÂ ","2020"
"Graph learning: A survey","IEEE Transactions onÂ ","2021"
"Ogb-lsc: A large-scale challenge for machine learning on graphs","arXiv preprint arXivÂ ","2021"
"Membership inference attacks on machine learning: A survey","ACM Computing SurveysÂ ","2022"
"TopFormer: Token pyramid transformer for mobile semantic segmentation","Proceedings of theÂ ","2022"
"Multi-agent reinforcement learning: A selective overview of theories and algorithms","Handbook of reinforcement learning andÂ ","2021"
"Vibe: Video inference for human body pose and shape estimation","Proceedings of the IEEEÂ ","2020"
"Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks","IEEE transactions on pattern analysis andÂ ","2021"
"Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding","Proceedings of The Web ConferenceÂ ","2020"
"Wayformer: Motion forecasting via simple & efficient attention networks","Â on Robotics andÂ ","2023"
"Cross-lingual language model pretraining","Advances in neural informationÂ ","2019"
"[HTML][HTML] Temporal fusion transformers for interpretable multi-horizon time series forecasting","International Journal of Forecasting","2021"
" Language models of protein sequences at the scale of evolution enable accurate structure prediction","BioRxiv","2022"
"Swformer: Sparse window transformer for 3d object detection in point clouds","Â on Computer Vision","2022"
"Retrieval augmentation reduces hallucination in conversation","arXiv preprint arXivÂ ","2021"
"How much knowledge can you pack into the parameters of a language model?","arXiv preprint arXiv:2002.08910","2020"
"Audiolm: a language modeling approach to audio generation","Â on Audio, SpeechÂ ","2023"
"Transformer models for text-based emotion detection: a review of BERT-based approaches","Artificial IntelligenceÂ ","2021"
"Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows","Proceedings of the IEEEÂ ","2022"
"Attention is all you need in speech separation","ICASSP 2021-2021Â ","2021"
"An introduction to deep learning in natural language processing: Models, techniques, and tools","Neurocomputing","2022"
"Time series data augmentation for deep learning: A survey","arXiv preprint arXivÂ ","2020"
"How can we know what language models know?","Transactions of the Association forÂ ","2020"
"Self-supervised multi-channel hypergraph convolutional network for social recommendation","Proceedings of the webÂ ","2021"
"A survey of controllable text generation using transformer-based pre-trained language models","ACM Computing Surveys","2023"
"Efficient self-supervised vision transformers for representation learning","arXiv preprint arXivÂ ","2021"
"Dialogpt: Large-scale generative pre-training for conversational response generation","arXiv preprint arXivÂ ","2019"
"Pushing the limits of semi-supervised learning for automatic speech recognition","arXiv preprint arXivÂ ","2020"
"Gman: A graph multi-attention network for traffic prediction","Proceedings of the AAAI conference onÂ ","2020"
"Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale","InternationalÂ ","2022"
"Declutr: Deep contrastive learning for unsupervised textual representations","arXiv preprint arXiv:2006.03659","2020"
"Transformers: State-of-the-art natural language processing","Proceedings of theÂ ","2020"
"[HTML][HTML] Combined scaling for zero-shot transfer learning","Neurocomputing","2023"
"Speaker recognition based on deep learning: An overview","Neural Networks","2021"
"Transvg: End-to-end visual grounding with transformers","Proceedings of the IEEEÂ ","2021"
"Social interactions for autonomous driving: A review and perspectives","Foundations and TrendsÂ ","2022"
"Adapterhub: A framework for adapting transformers","arXiv preprint arXivÂ ","2020"
"Huggingface's transformers: State-of-the-art natural language processing","arXiv preprint arXivÂ ","2019"
"Transformer memory as a differentiable search index","Advances inÂ ","2022"
"COVIDSenti: A large-scale benchmark Twitter data set for COVID-19 sentiment analysis","IEEE transactions onÂ ","2021"
"Virtex: Learning visual representations from textual annotations","Â of the IEEE/CVF conference onÂ ","2021"
"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","arXiv preprint arXiv:1910.01108","2019"
"[HTML][HTML] The forward physics facility at the high-luminosity LHC","Journal of Physics GÂ ","2023"
"Temporal graph networks for deep learning on dynamic graphs","arXiv preprint arXivÂ ","2020"
"Hyper-parameter optimization: A review of algorithms and applications","arXiv preprint arXiv:2003.05689","2020"
"VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation","arXiv preprint arXivÂ ","2021"
"Actbert: Learning global-local video-text representations","Proceedings of the IEEE/CVF conferenceÂ ","2020"
"A survey of the state of explainable AI for natural language processing","arXiv preprint arXivÂ ","2020"
"Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks","International Conference onÂ ","2021"
"Reformer: The efficient transformer","arXiv preprint arXiv:2001.04451","2020"
"Simpler is better: Few-shot semantic segmentation with classifier weight transformer","Proceedings of theÂ ","2021"
"Mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction","Proceedings of theÂ ","2022"
"Electra: Pre-training text encoders as discriminators rather than generators","arXiv preprint arXiv:2003.10555","2020"
"Albert: A lite bert for self-supervised learning of language representations","arXiv preprint arXivÂ ","2019"
"Deep image deblurring: A survey","International Journal ofÂ ","2022"
"Understanding and mitigating gradient flow pathologies in physics-informed neural networks","SIAM Journal on Scientific Computing","2021"
"Joint feature learning and relation modeling for tracking: A one-stream framework","European Conference onÂ ","2022"
"Anticipative video transformer","Proceedings of the IEEE/CVFÂ ","2021"
"Learning lane graph representations for motion forecasting","Computer Visionâ€“ECCVÂ ","2020"
"Extractive summarization as text matching","arXiv preprint arXivÂ ","2020"
"Styleswin: Transformer-based gan for high-resolution image generation","Proceedings of theÂ ","2022"
"Object-contextual representations for semantic segmentation","Computer Visionâ€“ECCV 2020: 16th EuropeanÂ ","2020"
"Heterofl: Computation and communication efficient federated learning for heterogeneous clients","arXiv preprint arXiv:2010.01264","2020"
"Tip-adapter: Training-free clip-adapter for better vision-language modeling","arXiv preprint arXivÂ ","2021"
"Tinybert: Distilling bert for natural language understanding","arXiv preprint arXivÂ ","2019"
"The lottery ticket hypothesis for pre-trained bert networks","Advances in neuralÂ ","2020"
"Semi-supervised city-wide parking availability prediction via hierarchical recurrent graph neural network","IEEE Transactions onÂ ","2020"
"Multi-class token transformer for weakly supervised semantic segmentation","Proceedings of theÂ ","2022"
"Template-based named entity recognition using BART","arXiv preprint arXiv:2106.01760","2021"
"A survey on evaluation of large language models","arXiv preprint arXivÂ ","2023"
"Codegen: An open large language model for code with multi-turn program synthesis","arXiv preprint arXivÂ ","2022"
"Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition","IEEE Journal ofÂ ","2022"
"Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram","ICASSP 2020-2020 IEEEÂ ","2020"
"Multiscale Vision Transformers.","ICCV","2021"
"Dive into deep learning","arXiv preprint arXiv:2106.11342","2021"
"Semantic communication systems for speech transmission","IEEE Journal on Selected Areas inÂ ","2021"
"A survey of android malware detection with deep neural models","ACM Computing SurveysÂ ","2020"
"Uniter: Universal image-text representation learning","European conference onÂ ","2020"
"From show to tell: A survey on deep learning-based image captioning","IEEE transactions onÂ ","2022"
"TaBERT: Pretraining for joint understanding of textual and tabular data","arXiv preprint arXiv:2005.08314","2020"
"Masked world models for visual control","Â on Robot Learning","2023"
"Sentence-bert: Sentence embeddings using siamese bert-networks","arXiv preprint arXiv:1908.10084","2019"
"Few-shot incremental learning with continually evolved classifiers","Proceedings of theÂ ","2021"
"[HTML][HTML] A survey of deep meta-learning","Artificial Intelligence Review","2021"
"Multimodal learning with transformers: A survey","IEEE Transactions on Pattern AnalysisÂ ","2023"
"Towards large-scale small object detection: Survey and benchmarks","Â on Pattern AnalysisÂ ","2023"
"Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization","Proceedings of theÂ ","2022"
"Deep double descent: Where bigger models and more data hurt","Journal of StatisticalÂ ","2021"
"Surface representation for point clouds","Â of the IEEE/CVF Conference onÂ ","2022"
"Frozen clip models are efficient video learners","Â on Computer Vision","2022"
"Deep learning-based human pose estimation: A survey","ACM ComputingÂ ","2023"
"Lxmert: Learning cross-modality encoder representations from transformers","arXiv preprint arXiv:1908.07490","2019"
"[HTML][HTML] Deep learning for cardiac image segmentation: a review","Frontiers inÂ ","2020"
"Sign language transformers: Joint end-to-end sign language recognition and translation","Proceedings of theÂ ","2020"
"Focal attention for long-range interactions in vision transformers","Advances in NeuralÂ ","2021"
"Text summarization with pretrained encoders","arXiv preprint arXiv:1908.08345","2019"
"Vl-bert: Pre-training of generic visual-linguistic representations","arXiv preprint arXivÂ ","2019"
"Tokenlearner: Adaptive space-time tokenization for videos","Advances inÂ ","2021"
"Efficiently modeling long sequences with structured state spaces","arXiv preprint arXiv:2111.00396","2021"
"Ctrl: A conditional transformer language model for controllable generation","arXiv preprint arXivÂ ","2019"
"Unified vision-language pre-training for image captioning and vqa","Proceedings of the AAAIÂ ","2020"
"Resunet++: An advanced architecture for medical image segmentation","Â on multimedia (ISM)","2019"
"Global context enhanced graph neural networks for session-based recommendation","Proceedings of the 43rdÂ ","2020"
" BERTERS: Multimodal Representation Learning for Expert Recommendation System with Transformer","arXiv preprint arXivÂ ","2020"
"Hero: Hierarchical encoder for video+ language omni-representation pre-training","arXiv preprint arXivÂ ","2020"
"Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks","Advances in neuralÂ ","2019"
"Movinets: Mobile video networks for efficient video recognition","Proceedings of theÂ ","2021"
"Autosdf: Shape priors for 3d completion, reconstruction and generation","Proceedings of theÂ ","2022"
"On the variance of the adaptive learning rate and beyond","arXiv preprint arXivÂ ","2019"
"Deep graph library: A graph-centric, highly-performant package for graph neural networks","arXiv preprint arXivÂ ","2019"
"A survey on instance segmentation: state of the art","International journal of multimedia informationÂ ","2020"
" Pretrained transformers as universal computation engines","arXiv preprint arXiv:2103.05247","2021"
"DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome","Bioinformatics","2021"
"Inductive biases for deep learning of higher-level cognition","Proceedings of the Royal Society A","2022"
"Roberta: A robustly optimized bert pretraining approach","arXiv preprint arXivÂ ","2019"
"Representing long-range context for graph neural networks with global attention","Advances inÂ ","2021"
"No fear of heterogeneity: Classifier calibration for federated learning with non-iid data","Advances in NeuralÂ ","2021"
"Spatio-temporal graph transformer networks for pedestrian trajectory prediction","Â , Glasgow, UK, August 23â€“28, 2020Â ","2020"
"Reinforcement learning for combinatorial optimization: A survey","Computers & OperationsÂ ","2021"
"On layer normalization in the transformer architecture","InternationalÂ ","2020"
"Associating objects with transformers for video object segmentation","Advances in Neural InformationÂ ","2021"
"Attention on attention for image captioning","Proceedings of the IEEEÂ ","2019"
"Persistent anti-muslim bias in large language models","Proceedings of the 2021 AAAI/ACMÂ ","2021"
"[HTML][HTML] Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction","NPJ digital medicine","2021"
"Tokenpose: Learning keypoint tokens for human pose estimation","Proceedings of theÂ ","2021"
"Emergent tool use from multi-agent autocurricula","arXiv preprint arXivÂ ","2019"
"A comparative study on transformer vs rnn in speech applications","2019 IEEE AutomaticÂ ","2019"
"Explainable deep learning: A field guide for the uninitiated","Journal of Artificial IntelligenceÂ ","2022"
"A simple framework for open-vocabulary segmentation and detection","Proceedings of theÂ ","2023"
"K-bert: Enabling language representation with knowledge graph","Proceedings of theÂ ","2020"
"Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures","Advances in NeuralÂ ","2022"
"Neural collaborative filtering vs. matrix factorization revisited","Proceedings of the 14thÂ ","2020"
"Grandmaster level in StarCraft II using multi-agent reinforcement learning","Nature","2019"
"[HTML][HTML] A literature survey of recent advances in chatbots","Information","2022"
"Knowledge enhanced contextual word representations","arXiv preprint arXivÂ ","2019"
"Hift: Hierarchical feature transformer for aerial tracking","Proceedings of the IEEE/CVFÂ ","2021"
"nnformer: Interleaved transformer for volumetric segmentation","arXiv preprint arXivÂ ","2021"
"12-in-1: Multi-task vision and language representation learning","Proceedings of theÂ ","2020"
"Glow-tts: A generative flow for text-to-speech via monotonic alignment search","Advances in NeuralÂ ","2020"
"Regtr: End-to-end point cloud correspondences with transformers","Proceedings of the IEEE/CVF conferenceÂ ","2022"
"Learning knowledge graph embedding with heterogeneous relation attention networks","IEEE Transactions onÂ ","2021"
"Jukebox: A generative model for music","arXiv preprint arXivÂ ","2020"
"A survey on generative diffusion model","arXiv preprint arXivÂ ","2022"
"What can transformers learn in-context? a case study of simple function classes","Advances in NeuralÂ ","2022"
"[HTML][HTML] Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals","NatureÂ ","2020"
"Jkt: A joint graph convolutional network based deep knowledge tracing","Information Sciences","2021"
"[HTML][HTML] A large language model for electronic health records","NPJ DigitalÂ ","2022"
"ERNIE-ViLG 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts","Proceedings of theÂ ","2023"
"Masked label prediction: Unified message passing model for semi-supervised classification","arXiv preprint arXivÂ ","2020"
"Xlnet: Generalized autoregressive pretraining for language understanding","Advances in neuralÂ ","2019"
"Qplex: Duplex dueling multi-agent q-learning","arXiv preprint arXiv:2008.01062","2020"
"All tokens matter: Token labeling for training better vision transformers","Advances in neuralÂ ","2021"
"Minivit: Compressing vision transformers with weight multiplexing","Proceedings of theÂ ","2022"
"Disentangled graph collaborative filtering","Proceedings of the 43rdÂ ","2020"
"Rethinking space-time networks with improved memory coverage for efficient video object segmentation","Advances in NeuralÂ ","2021"
"Asymmetric non-local neural networks for semantic segmentation","Proceedings of the IEEEÂ ","2019"
"FLAT: Chinese NER using flat-lattice transformer","arXiv preprint arXiv:2004.11795","2020"
"Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics","arXiv preprint arXiv:2104.13346","2021"
"Software vulnerability detection using deep neural networks: a survey","Proceedings of theÂ ","2020"
"Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting","Advances in neuralÂ ","2019"
"Ernie-vil: Knowledge enhanced vision-language representations through scene graphs","Proceedings of the AAAIÂ ","2021"
"Text2Event: Controllable sequence-to-structure generation for end-to-end event extraction","arXiv preprint arXivÂ ","2021"
"Exploring the limits of transfer learning with a unified text-to-text transformer","The Journal of MachineÂ ","2020"
"MCVD-masked conditional video diffusion for prediction, generation, and interpolation","Advances in NeuralÂ ","2022"
"Chasing sparsity in vision transformers: An end-to-end exploration","Advances in NeuralÂ ","2021"
"An explanation of in-context learning as implicit bayesian inference","arXiv preprint arXiv:2111.02080","2021"
"vq-wav2vec: Self-supervised learning of discrete speech representations","arXiv preprint arXiv:1910.05453","2019"
"Class-incremental learning by knowledge distillation with adaptive feature consolidation","Â of the IEEE/CVF conference onÂ ","2022"
"Ernie 2.0: A continual pre-training framework for language understanding","Proceedings of the AAAIÂ ","2020"
"Energy and policy considerations for deep learning in NLP","arXiv preprint arXiv:1906.02243","2019"
"Pretrained transformers improve out-of-distribution robustness","arXiv preprint arXivÂ ","2020"
"On the cross-lingual transferability of monolingual representations","arXiv preprint arXiv:1910.11856","2019"
"Pose recognition with cascade transformers","Proceedings of theÂ ","2021"
"AdapterFusion: Non-destructive task composition for transfer learning","arXiv preprint arXivÂ ","2020"
"Can language models learn from explanations in context?","arXiv preprint arXivÂ ","2022"
"Masked discrimination for self-supervised learning on point clouds","European Conference on Computer Vision","2022"
"Structure-aware transformer for graph representation learning","Â Conference on MachineÂ ","2022"
"SCF-Net: Learning spatial contextual features for large-scale point cloud segmentation","Proceedings of theÂ ","2021"
"Learning affinity from attention: End-to-end weakly-supervised semantic segmentation with transformers","Â of the IEEE/CVF Conference onÂ ","2022"
"Deep equilibrium models","Advances in Neural InformationÂ ","2019"
"Lookahead optimizer: k steps forward, 1 step back","Advances in neuralÂ ","2019"
"Coco-lm: Correcting and contrasting text sequences for language model pretraining","Advances in NeuralÂ ","2021"
"When does label smoothing help?","Advances in neuralÂ ","2019"
"Plug and play language models: A simple approach to controlled text generation","arXiv preprint arXivÂ ","2019"
"In-place scene labelling and understanding with implicit scene representation","Proceedings of theÂ ","2021"
"Findings of the 2019 conference on machine translation (WMT19)","L Barrault, O Bojar, MR Costa-Jussa, C Federmann - 2019 - zora.uzh.ch","2019"
"Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss","ICASSP 2020-2020Â ","2020"
"Language models as knowledge bases?","arXiv preprint arXivÂ ","2019"
"Griddehazenet: Attention-based multi-scale network for image dehazing","Proceedings of the IEEE/CVFÂ ","2019"
"Shunted self-attention via multi-scale token aggregation","Proceedings of the IEEEÂ ","2022"
"A unified generative framework for various NER subtasks","arXiv preprint arXivÂ ","2021"
"What does bert look at? an analysis of bert's attention","arXiv preprint arXivÂ ","2019"
"General multi-label image classification with transformers","Proceedings of theÂ ","2021"
"Violet: End-to-end video-language transformers with masked visual-token modeling","arXiv preprint arXivÂ ","2021"
"KEPLER: A unified model for knowledge embedding and pre-trained language representation","Transactions of theÂ ","2021"
"Towards grand unification of object tracking","European Conference onÂ ","2022"
"Csdi: Conditional score-based diffusion models for probabilistic time series imputation","Advances in NeuralÂ ","2021"
"Neural graph collaborative filtering","Proceedings of the 42ndÂ ","2019"
"Hopfield networks is all you need","arXiv preprint arXivÂ ","2020"
"Dual-level collaborative transformer for image captioning","Proceedings of theÂ ","2021"
"Srdiff: Single image super-resolution with diffusion probabilistic models","Neurocomputing","2022"
"Transformer-based multimodal information fusion for facial expression analysis","Proceedings of theÂ ","2022"
"Unilmv2: Pseudo-masked language models for unified language model pre-training","InternationalÂ ","2020"
"Transweather: Transformer-based restoration of images degraded by adverse weather conditions","Proceedings of the IEEEÂ ","2022"
"K-adapter: Infusing knowledge into pre-trained models with adapters","arXiv preprint arXivÂ ","2020"
"[HTML][HTML] SpookyNet: Learning force fields with electronic degrees of freedom and nonlocal effects","NatureÂ ","2021"
"Deep learning for monocular depth estimation: A review","Neurocomputing","2021"
"Multivariate time-series anomaly detection via graph attention network","Â Conference on DataÂ ","2020"
"Evaluating the factual consistency of abstractive text summarization","arXiv preprint arXivÂ ","2019"
"Polarformer: Multi-camera 3d object detection with polar transformer","Proceedings of theÂ ","2023"
"Interactive language: Talking to robots in real time","IEEE Robotics andÂ ","2023"
"Tip-adapter: Training-free adaption of clip for few-shot classification","Â on Computer Vision","2022"
"Applications of deep learning in stock market prediction: recent progress","Expert Systems with Applications","2021"
"On the relationship between self-attention and convolutional layers","arXiv preprint arXiv:1911.03584","2019"
"Generalized decoding for pixel, image, and language","Proceedings of theÂ ","2023"
"What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models","Transactions of the Association for ComputationalÂ ","2020"
"Kgat: Knowledge graph attention network for recommendation","Proceedings of the 25th ACMÂ ","2019"
"Bertscore: Evaluating text generation with bert","arXiv preprint arXivÂ ","2019"
"Transfer: Learning relation-aware facial expression representations with transformers","Proceedings of the IEEE/CVFÂ ","2021"
"Unsupervised data augmentation for consistency training","Advances in neuralÂ ","2020"
"ProGen2: exploring the boundaries of protein language models","Cell Systems","2022"
"Editing conditional radiance fields","Proceedings of theÂ ","2021"
"UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery","ISPRS Journal ofÂ ","2022"
"Text and code embeddings by contrastive pre-training","arXiv preprint arXivÂ ","2022"
"Pixel-bert: Aligning image pixels with text by deep multi-modal transformers","arXiv preprint arXiv:2004.00849","2020"
"ERNIE: Enhanced language representation with informative entities","arXiv preprint arXivÂ ","2019"
"Unified language model pre-training for natural language understanding and generation","Advances in neuralÂ ","2019"
"The curious case of neural text degeneration","arXiv preprint arXivÂ ","2019"
"BERT rediscovers the classical NLP pipeline","arXiv preprint arXiv:1905.05950","2019"
"Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors","Proceedings of theÂ ","2021"
"Deep hierarchical semantic segmentation","Proceedings of the IEEEÂ ","2022"
"Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training","arXiv preprint arXivÂ ","2021"
"Iterative deep graph learning for graph neural networks: Better and robust node embeddings","Advances in neural informationÂ ","2020"
"Gcnet: Non-local networks meet squeeze-excitation networks and beyond","Proceedings of the IEEEÂ ","2019"
"[HTML][HTML] A high-performance speech neuroprosthesis","Nature","2023"
"fairseq: A fast, extensible toolkit for sequence modeling","arXiv preprint arXivÂ ","2019"
"Zero: Memory optimizations toward training trillion parameter models","Â Conference for HighÂ ","2020"
"Pre-training with whole word masking for chinese bert","IEEE/ACM Transactions onÂ ","2021"
"Mls: A large-scale multilingual dataset for speech research","arXiv preprint arXivÂ ","2020"
"SciBERT: A pretrained language model for scientific text","arXiv preprint arXiv:1903.10676","2019"
"Finbert: Financial sentiment analysis with pre-trained language models","arXiv preprint arXiv:1908.10063","2019"
"Clip2video: Mastering video-text retrieval via image clip","arXiv preprint arXiv:2106.11097","2021"
"Deep anomaly detection for time-series data in industrial IoT: A communication-efficient on-device federated learning approach","IEEE Internet ofÂ ","2020"
"DASNet: Dual attentive fully convolutional Siamese networks for change detection in high-resolution satellite images","IEEE Journal ofÂ ","2020"
"I-bert: Integer-only bert quantization","Â on machine learning","2021"
"Abd-net: Attentive but diverse person re-identification","Proceedings of theÂ ","2019"
"ERASER: A benchmark to evaluate rationalized NLP models","arXiv preprint arXivÂ ","2019"
"Analog bits: Generating discrete data using diffusion models with self-conditioning","arXiv preprint arXiv:2208.04202","2022"
"GFlowNet-EM for learning compositional latent variable models","InternationalÂ ","2023"
"Expectation-maximization attention networks for semantic segmentation","Proceedings of theÂ ","2019"
"Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training","arXiv preprint arXivÂ ","2020"
"Mining cross-image semantics for weakly supervised semantic segmentation","Â , Glasgow, UK, August 23â€“28, 2020Â ","2020"
"[HTML][HTML] Transformers in medical image analysis","IntelligentÂ ","2023"
"Optimizing dense retrieval model training with hard negatives","Proceedings of the 44thÂ ","2021"
"Lost in the middle: How language models use long contexts","arXiv preprint arXivÂ ","2023"
"Graph convolutional networks for temporal action localization","Proceedings of theÂ ","2019"
"Lavt: Language-aware vision transformer for referring image segmentation","Proceedings of theÂ ","2022"
"Masked visual pre-training for motor control","arXiv preprint arXiv:2203.06173","2022"
"Tinyvit: Fast pretraining distillation for small vision transformers","European Conference onÂ ","2022"
"Rpm-net: Robust point matching using learned features","Proceedings of the IEEE/CVF conferenceÂ ","2020"
"Mass: Masked sequence to sequence pre-training for language generation","arXiv preprint arXiv:1905.02450","2019"
"Oneformer: One transformer to rule universal image segmentation","Proceedings of theÂ ","2023"
"Experience grounds language","arXiv preprint arXivÂ ","2020"
"Savi++: Towards end-to-end object-centric learning from real-world videos","Advances inÂ ","2022"
" Language models are unsupervised multitask learners","OpenAIÂ ","2019"
"Flaubert: Unsupervised language model pre-training for french","arXiv preprint arXivÂ ","2019"
"BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer","Proceedings of the 28thÂ ","2019"
"[HTML][HTML] Brains and algorithms partially converge in natural language processing","Communications biology","2022"
"Align your latents: High-resolution video synthesis with latent diffusion models","Proceedings of theÂ ","2023"
"Support-set bottlenecks for video-text representation learning","arXiv preprint arXivÂ ","2020"
"Identifying facemask-wearing condition using image super-resolution with classification network to prevent COVID-19","Sensors","2020"
"Vision-language transformer and query generation for referring segmentation","Proceedings of the IEEEÂ ","2021"
"Green ai","Communications of the ACM","2020"
"Fastspeech: Fast, robust and controllable text to speech","Advances in neuralÂ ","2019"
"Videobert: A joint model for video and language representation learning","Proceedings of theÂ ","2019"
"Disentangled non-local neural networks","Computer Visionâ€“ECCVÂ ","2020"
"Bipartite graph network with adaptive message passing for unbiased scene graph generation","Proceedings of the IEEE/CVFÂ ","2021"
"Effective energy consumption forecasting using empirical wavelet transform and long short-term memory","Energy","2022"
"Crossformer++: A versatile vision transformer hinging on cross-scale attention","arXiv preprint arXivÂ ","2023"
"Efficient self-supervised learning with contextualized target representations for vision, speech and language","Â Conference on MachineÂ ","2023"
"Episodic transformer for vision-and-language navigation","Proceedings of the IEEEÂ ","2021"
"How to fine-tune bert for text classification?","Â 18th China National Conference, CCL 2019Â ","2019"
"End-to-end human object interaction detection with hoi transformer","Proceedings of theÂ ","2021"
"Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned","arXiv preprint arXivÂ ","2019"
"Large language models can be strong differentially private learners","arXiv preprint arXiv:2110.05679","2021"
"A federated learning system with enhanced feature extraction for human activity recognition","Knowledge-Based Systems","2021"
"[HTML][HTML] Multimodal transformer for unaligned multimodal language sequences","Proceedings of theÂ ","2019"
"BioBERT: a pre-trained biomedical language representation model for biomedical text mining","J Lee, W Yoon, S Kim, D Kim, S Kim, CH SoÂ - , 2020 - academic.oup.com","2020"
"Time interval aware self-attention for sequential recommendation","Â of the 13th international conference on webÂ ","2020"
"Videogpt: Video generation using vq-vae and transformers","arXiv preprint arXiv:2104.10157","2021"
"Graph neural networks: foundation, frontiers and applications","Â of the 28th ACM SIGKDD ConferenceÂ ","2022"
"Utilizing graph machine learning within drug discovery and development","Briefings inÂ ","2021"
"Cure: Code-aware neural machine translation for automatic program repair","2021 IEEE/ACM 43rd InternationalÂ ","2021"
"Memory-augmented dense predictive coding for video representation learning","European conference on computer vision","2020"
"Human motion diffusion model","arXiv preprint arXivÂ ","2022"
"Edvr: Video restoration with enhanced deformable convolutional networks","Proceedings of theÂ ","2019"
"Tranad: Deep transformer networks for anomaly detection in multivariate time series data","arXiv preprint arXiv:2201.07284","2022"
"Probabilistic two-stage detection","arXiv preprint arXiv:2103.07461","2021"
"Avatarclip: Zero-shot text-driven generation and animation of 3d avatars","arXiv preprint arXivÂ ","2022"
"Multiple object tracking with correlation learning","Proceedings of the IEEEÂ ","2021"
"Retrieving and reading: A comprehensive survey on open-domain question answering","arXiv preprint arXivÂ ","2021"
"Vision transformers for single image dehazing","IEEE Transactions on ImageÂ ","2023"
"Spatial-spectral transformer for hyperspectral image classification","Remote Sensing","2021"
"Efficient training of visual transformers with small datasets","Advances in NeuralÂ ","2021"
"Heterogeneous graph attention network","The world wide webÂ ","2019"
"Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis","Proceedings of the AAAI conference onÂ ","2021"
"Parameter-efficient transfer learning for NLP","InternationalÂ ","2019"
"Spanbert: Improving pre-training by representing and predicting spans","Transactions of theÂ ","2020"
"Large-scale long-tailed recognition in an open world","Proceedings of theÂ ","2019"
"Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset","Proceedings of the IEEE/CVFÂ ","2021"
"Interventional few-shot learning","Advances in neuralÂ ","2020"
"Learning to dispatch for job shop scheduling via deep reinforcement learning","Advances in NeuralÂ ","2020"
"Ernie: Enhanced representation through knowledge integration","arXiv preprint arXivÂ ","2019"
"Few-shot object detection with fully cross-transformer","Proceedings of theÂ ","2022"
"Attention is not explanation","arXiv preprint arXiv:1902.10186","2019"
"Are NLP models really able to solve simple math word problems?","arXiv preprint arXiv:2103.07191","2021"
"Q-bert: Hessian based ultra low precision quantization of bert","Proceedings of theÂ ","2020"
"Are sixteen heads really better than one?","Advances in neuralÂ ","2019"
"A transformer-based approach for source code summarization","arXiv preprint arXivÂ ","2020"
"Unsupervised cross-lingual representation learning at scale","arXiv preprint arXivÂ ","2019"
"Incremental transformer structure enhanced image inpainting with masking positional encoding","Â of the IEEE/CVF Conference onÂ ","2022"
"Similarity of neural network representations revisited","Â conference on machineÂ ","2019"
"Machine learning for high-entropy alloys: Progress, challenges and opportunities","Progress in Materials Science","2023"
"3d infomax improves gnns for molecular property prediction","InternationalÂ ","2022"
"Flexible diffusion modeling of long videos","Advances inÂ ","2022"
"Uncertainty-guided transformer reasoning for camouflaged object detection","Proceedings of theÂ ","2021"
"Specter: Document-level representation learning using citation-informed transformers","arXiv preprint arXivÂ ","2020"
"Cross-lingual language model pretraining","arXiv preprint arXiv:1901.07291","2019"
"[HTML][HTML] Graph neural networks: A review of methods and applications","AI open","2020"
" Graph contextualized self-attention network for session-based recommendation.","IJCAI","2019"
"Misa: Modality-invariant and-specific representations for multimodal sentiment analysis","Proceedings of the 28th ACMÂ ","2020"
"Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism","Journal of medicinalÂ ","2019"
"Movement pruning: Adaptive sparsity by fine-tuning","Advances in Neural InformationÂ ","2020"
"Deep modular co-attention networks for visual question answering","Proceedings of the IEEEÂ ","2019"
"Anomaly transformer: Time series anomaly detection with association discrepancy","arXiv preprint arXiv:2110.02642","2021"
"Q8bert: Quantized 8bit bert","2019 Fifth Workshop onÂ ","2019"
"Generalized category discovery","Proceedings of theÂ ","2022"
"Visformer: The vision-friendly transformer","Proceedings of theÂ ","2021"
"Detrs with hybrid matching","Proceedings of theÂ ","2023"
"Is attention interpretable?","arXiv preprint arXiv:1906.03731","2019"
"Are we done with imagenet?","arXiv preprint arXivÂ ","2020"
"On generative spoken language modeling from raw audio","Transactions of theÂ ","2021"
"Fast point transformer","Proceedings of the IEEEÂ ","2022"
"Time-series representation learning via temporal and contextual contrasting","arXiv preprint arXivÂ ","2021"
"Nerv: Neural representations for videos","Advances in NeuralÂ ","2021"
"Learning and evaluating contextual embedding of source code","Â on machine learning","2020"
"Transmorph: Transformer for unsupervised medical image registration","Medical image analysis","2022"
"Skeleton-based action recognition with multi-stream adaptive graph convolutional networks","IEEE Transactions on ImageÂ ","2020"
"BioSeq-BLM: a platform for analyzing DNA, RNA and protein sequences based on biological language models","Nucleic acids research","2021"
