{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그림그리는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        ↑       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←  →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        ↓       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [2,2]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road,road],\n",
    "                   [road,road,road],\n",
    "                   [road,road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"road\"],\n",
    "                    [\"road\",\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인    \n",
    "        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 상태 가치함수를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 가치 계산\n",
    "#G는 수익,max_step: epoch\n",
    "def state_value_function(env,agent,G,max_step,now_step):\n",
    "    \n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "# 2. 현재 위치가 도착지점인지 확인\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "    \n",
    "# 3. 마지막 상태는 보상만 계산\n",
    "    if (max_step == now_step):\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 3.1 가능한 모든 행동의 보상을 계산\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            observation, reward, done = env.move(agent,i)\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "            \n",
    "        return G\n",
    "    \n",
    "    # 4. 현재 상태의 보상을 계산한 후 다음 step으로 이동\n",
    "    else:\n",
    "        \n",
    "        # 4.1현재 위치 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.2 현재 위치에서 가능한 모든 행동을 조사한 후 이동\n",
    "        for i in range(len(agent.action)):\n",
    "            observation, reward, done = env.move(agent,i)      \n",
    "            # 4.2.1 현재 상태에서 보상을 계산\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "\n",
    "            # 4.2.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n",
    "            if done == True:\n",
    "                if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                    agent.set_pos(pos1)\n",
    "\n",
    "            # 4.2.3 다음 step을 계산\n",
    "            next_v = state_value_function(env, agent, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v\n",
    "\n",
    "            # 4.2.4 현재 위치를 복구\n",
    "            agent.set_pos(pos1)\n",
    "\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미로의 각 상태의 상태가치함수를 구하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 0 total_time = 0.01(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.00      |      -1.50      |      -2.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -1.50      |      -1.00      |      -1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.00      |      -1.00      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 1 total_time = 0.0(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.58      |      -2.96      |      -3.46      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -2.96      |      -2.12      |      -1.68      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -3.46      |      -1.68      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 2 total_time = 0.02(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.94      |      -4.23      |      -4.60      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.23      |      -3.09      |      -2.41      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -4.60      |      -2.41      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 3 total_time = 0.02(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.13      |      -5.29      |      -5.56      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -5.29      |      -3.99      |      -3.05      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -5.56      |      -3.05      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 4 total_time = 0.04(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.14      |      -6.22      |      -6.38      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.22      |      -4.75      |      -3.61      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -6.38      |      -3.61      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 5 total_time = 0.1(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.01      |      -7.01      |      -7.08      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.01      |      -5.42      |      -4.09      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.08      |      -4.09      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 6 total_time = 0.54(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.76      |      -7.69      |      -7.69      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.69      |      -6.00      |      -4.51      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -7.69      |      -4.51      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "max_step_number = 7 total_time = 1.99(s)\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -9.40      |      -8.28      |      -8.20      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.28      |      -6.49      |      -4.87      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|      -8.20      |      -4.87      |       1.00      |\n",
      "|                 |                 |                 |\n",
      "+-----------------+-----------------+-----------------+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Workspace\\SNA\\JupyterNotebook\\Reinforcement learning\\Grid_world.ipynb 셀 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(env\u001b[39m.\u001b[39mreward\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         agent\u001b[39m.\u001b[39mset_pos([i,j])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         v_table[i,j] \u001b[39m=\u001b[39m state_value_function(env,agent, \u001b[39m0\u001b[39m, max_step, \u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# 5.3 max_down에 따른 계산시간 저장\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m time_len\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_time)\n",
      "\u001b[1;32mc:\\Workspace\\SNA\\JupyterNotebook\\Reinforcement learning\\Grid_world.ipynb 셀 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         agent\u001b[39m.\u001b[39mset_pos(pos1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# 4.2.3 다음 step을 계산\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m next_v \u001b[39m=\u001b[39m state_value_function(env, agent, \u001b[39m0\u001b[39m, max_step, now_step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m G \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mselect_action_pr[i] \u001b[39m*\u001b[39m gamma \u001b[39m*\u001b[39m next_v\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# 4.2.4 현재 위치를 복구\u001b[39;00m\n",
      "\u001b[1;32mc:\\Workspace\\SNA\\JupyterNotebook\\Reinforcement learning\\Grid_world.ipynb 셀 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         agent\u001b[39m.\u001b[39mset_pos(pos1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# 4.2.3 다음 step을 계산\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m next_v \u001b[39m=\u001b[39m state_value_function(env, agent, \u001b[39m0\u001b[39m, max_step, now_step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m G \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mselect_action_pr[i] \u001b[39m*\u001b[39m gamma \u001b[39m*\u001b[39m next_v\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# 4.2.4 현재 위치를 복구\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: state_value_function at line 42 (5 times)]\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\SNA\\JupyterNotebook\\Reinforcement learning\\Grid_world.ipynb 셀 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         agent\u001b[39m.\u001b[39mset_pos(pos1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# 4.2.3 다음 step을 계산\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m next_v \u001b[39m=\u001b[39m state_value_function(env, agent, \u001b[39m0\u001b[39m, max_step, now_step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m G \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mselect_action_pr[i] \u001b[39m*\u001b[39m gamma \u001b[39m*\u001b[39m next_v\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# 4.2.4 현재 위치를 복구\u001b[39;00m\n",
      "\u001b[1;32mc:\\Workspace\\SNA\\JupyterNotebook\\Reinforcement learning\\Grid_world.ipynb 셀 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# 3.1 가능한 모든 행동의 보상을 계산\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(agent\u001b[39m.\u001b[39maction)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     agent\u001b[39m.\u001b[39mset_pos(pos1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     observation, reward, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mmove(agent,i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     G \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mselect_action_pr[i] \u001b[39m*\u001b[39m reward\n",
      "\u001b[1;32mc:\\Workspace\\SNA\\JupyterNotebook\\Reinforcement learning\\Grid_world.ipynb 셀 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# 4. 에이전트의 위치 저장\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_pos\u001b[39m(\u001b[39mself\u001b[39m,position):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m=\u001b[39m position\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/SNA/JupyterNotebook/Reinforcement%20learning/Grid_world.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "\n",
    "# 3. 최대 max_step_number 제한\n",
    "max_step_number = 10\n",
    "\n",
    "# 4. 계산 시간 저장을 위한 list\n",
    "time_len = []\n",
    "\n",
    "# 5. 재귀함수 state_value_function을를 이용해 각 상태 가치를 계산\n",
    "for max_step in range(max_step_number):\n",
    "    \n",
    "    # 5.1 미로 각 상태의 가치를 테이블 형식으로 저장\n",
    "    v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 5.2 미로의 각 상태에 대해 state_value_function() 을 이용해 가치를 계산한 후 테이블 형식으로 저장\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            agent.set_pos([i,j])\n",
    "            v_table[i,j] = state_value_function(env,agent, 0, max_step, 0)\n",
    "            \n",
    "    # 5.3 max_down에 따른 계산시간 저장\n",
    "    time_len.append(time.time()-start_time)\n",
    "    print(\"max_step_number = {} total_time = {}(s)\".format(max_step, np.round(time.time()-start_time,2)))\n",
    "    #보상에서 멀어질수록 G가 감소함\n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# 6. step 별 계산 시간 그래프 그리기    \n",
    "plt.plot(time_len, 'o-k')\n",
    "plt.xlabel('max_down')\n",
    "plt.ylabel('time(s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 행동가치함수를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 가치 함수\n",
    "def action_value_function(env, agent, act, G, max_step, now_step):   \n",
    "    \n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # 2. 현재 위치가 목적지인지 확인\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "\n",
    "    # 3. 마지막 상태는 보상만 계산\n",
    "    if (max_step == now_step):\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += agent.select_action_pr[act]*reward\n",
    "        return G\n",
    "    \n",
    "    # 4. 현재 상태의 보상을 계산한 후 다음 행동과 함께 다음 step으로 이동\n",
    "    else:\n",
    "        # 4.1현재 위치 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += agent.select_action_pr[act] * reward\n",
    "        \n",
    "        # 4.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n",
    "        if done == True:            \n",
    "            if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                agent.set_pos(pos1)\n",
    "            \n",
    "        # 4.3 현재 위치를 다시 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.4 현재 위치에서 가능한 모든 행동을 선택한 후 이동\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            next_v = action_value_function(env, agent, i, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미로의 각 상태의 행동가치함수를 구하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step = 0\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.75       |     -0.75       |     -0.75       |\n",
      "| -0.75     -0.25 | -0.25     -0.25 | -0.25     -0.75 |\n",
      "|     -0.25       |     -0.25       |     -0.25       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.25       |     -0.25       |     -0.25       |\n",
      "| -0.75     -0.25 | -0.25     -0.25 | -0.25     -0.75 |\n",
      "|     -0.25       |     -0.25       |      0.25       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.25       |     -0.25       |      1.00       |\n",
      "| -0.75     -0.25 | -0.25      0.25 |  1.00      1.00 |\n",
      "|     -0.75       |     -0.75       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |      ←  →     |      ←         |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|        ↑       |        ↑       |                 |\n",
      "|          →     |      ←  →     |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|        ↑       |                 |        ↑       |\n",
      "|          →     |          →     |      ←  →     |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 1\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.20       |     -1.09       |     -1.20       |\n",
      "| -1.20     -0.59 | -0.70     -0.70 | -0.59     -1.20 |\n",
      "|     -0.59       |     -0.48       |     -0.48       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.70       |     -0.59       |     -0.70       |\n",
      "| -1.09     -0.48 | -0.59     -0.48 | -0.48     -0.98 |\n",
      "|     -0.70       |     -0.48       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.59       |     -0.48       |      1.00       |\n",
      "| -1.20     -0.48 | -0.70      1.15 |  1.00      1.00 |\n",
      "|     -1.20       |     -0.98       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑       |\n",
      "|          →     |          →     |      ←  →     |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 2\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.55       |     -1.42       |     -1.53       |\n",
      "| -1.55     -0.92 | -1.05     -1.03 | -0.92     -1.53 |\n",
      "|     -0.92       |     -0.73       |     -0.48       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.05       |     -0.92       |     -1.03       |\n",
      "| -1.42     -0.73 | -0.92     -0.48 | -0.73     -0.98 |\n",
      "|     -1.03       |     -0.48       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -0.92       |     -0.73       |      1.00       |\n",
      "| -1.53     -0.48 | -1.03      1.15 |  1.00      1.00 |\n",
      "|     -1.53       |     -0.98       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑       |\n",
      "|          →     |          →     |      ←  →     |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 3\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.86       |     -1.70       |     -1.75       |\n",
      "| -1.86     -1.20 | -1.36     -1.25 | -1.20     -1.75 |\n",
      "|     -1.20       |     -0.88       |     -0.61       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.36       |     -1.20       |     -1.25       |\n",
      "| -1.70     -0.88 | -1.20     -0.61 | -0.88     -1.11 |\n",
      "|     -1.25       |     -0.61       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.20       |     -0.88       |      1.00       |\n",
      "| -1.75     -0.61 | -1.25      1.15 |  1.00      1.00 |\n",
      "|     -1.75       |     -1.11       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑       |\n",
      "|          →     |          →     |      ←  →     |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 4\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.13       |     -1.92       |     -1.94       |\n",
      "| -2.13     -1.42 | -1.63     -1.44 | -1.42     -1.94 |\n",
      "|     -1.42       |     -1.06       |     -0.72       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.63       |     -1.42       |     -1.44       |\n",
      "| -1.92     -1.06 | -1.42     -0.72 | -1.06     -1.22 |\n",
      "|     -1.44       |     -0.72       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.42       |     -1.06       |      1.00       |\n",
      "| -1.94     -0.72 | -1.44      1.15 |  1.00      1.00 |\n",
      "|     -1.94       |     -1.22       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑       |\n",
      "|          →     |          →     |      ←  →     |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 5\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.35       |     -2.11       |     -2.11       |\n",
      "| -2.35     -1.61 | -1.85     -1.61 | -1.61     -2.11 |\n",
      "|     -1.61       |     -1.21       |     -0.83       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.85       |     -1.61       |     -1.61       |\n",
      "| -2.11     -1.21 | -1.61     -0.83 | -1.21     -1.33 |\n",
      "|     -1.61       |     -0.83       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.61       |     -1.21       |      1.00       |\n",
      "| -2.11     -0.83 | -1.61      1.15 |  1.00      1.00 |\n",
      "|     -2.11       |     -1.33       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑       |\n",
      "|          →     |          →     |      ←  →     |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 6\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.53       |     -2.27       |     -2.25       |\n",
      "| -2.53     -1.77 | -2.03     -1.75 | -1.77     -2.25 |\n",
      "|     -1.77       |     -1.35       |     -0.92       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.03       |     -1.77       |     -1.75       |\n",
      "| -2.27     -1.35 | -1.77     -0.92 | -1.35     -1.42 |\n",
      "|     -1.75       |     -0.92       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.77       |     -1.35       |      1.00       |\n",
      "| -2.25     -0.92 | -1.75      1.15 |  1.00      1.00 |\n",
      "|     -2.25       |     -1.42       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑       |\n",
      "|          →     |          →     |      ←  →     |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n",
      "max_step = 7\n",
      "Q - table\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.69       |     -2.42       |     -2.37       |\n",
      "| -2.69     -1.92 | -2.19     -1.87 | -1.92     -2.37 |\n",
      "|     -1.92       |     -1.46       |     -1.01       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -2.19       |     -1.92       |     -1.87       |\n",
      "| -2.42     -1.46 | -1.92     -1.01 | -1.46     -1.51 |\n",
      "|     -1.87       |     -1.01       |      1.15       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|     -1.92       |     -1.46       |      1.00       |\n",
      "| -2.37     -1.01 | -1.87      1.15 |  1.00      1.00 |\n",
      "|     -2.37       |     -1.51       |      1.00       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |                 |                 |\n",
      "|        ↓       |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |                 |\n",
      "|          →     |          →     |                 |\n",
      "|                 |        ↓       |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "|                 |                 |        ↑       |\n",
      "|          →     |          →     |      ←  →     |\n",
      "|                 |                 |        ↓       |\n",
      "+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 재귀적으로 행동의 가치를 계산\n",
    "\n",
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "np.random.seed(0)\n",
    "\n",
    "# 3. 현재부터 max_step 까지 계산\n",
    "max_step_number = 8\n",
    "\n",
    "# 4. 모든 상태에 대해\n",
    "for max_step in range(max_step_number):\n",
    "    # 4.1 미로 상의 모든 상태에서 가능한 행동의 가치를 저장할 테이블을 정의\n",
    "    print(\"max_step = {}\".format(max_step))\n",
    "    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            # 4.2 모든 행동에 대해\n",
    "            for action in range(len(agent.action)):\n",
    "                # 4.2.1 에이전트의 위치를 초기화\n",
    "                agent.set_pos([i,j])\n",
    "                # 4.2.2 현재 위치에서 행동 가치를 계산\n",
    "                q_table[i ,j,action] = action_value_function(env, agent, action, 0, max_step, 0)\n",
    "\n",
    "    q = np.round(q_table,2)\n",
    "    print(\"Q - table\")\n",
    "    show_q_table(q, env)\n",
    "    print(\"High actions Arrow\")\n",
    "    show_q_table_arrow(q,env)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
